{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Boosting a decision stump\n",
    "\n",
    "The goal of this notebook is to implement your own boosting module.\n",
    "\n",
    "**Brace yourselves**! This is going to be a fun and challenging assignment.\n",
    "\n",
    "\n",
    "* Use SFrames to do some feature engineering.\n",
    "* Modify the decision trees to incorporate weights.\n",
    "* Implement Adaboost ensembling.\n",
    "* Use your implementation of Adaboost to train a boosted decision stump ensemble.\n",
    "* Evaluate the effect of boosting (adding more decision stumps) on performance of the model.\n",
    "* Explore the robustness of Adaboost to overfitting.\n",
    "\n",
    "Let's get started!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fire up GraphLab Create"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Make sure you have the latest version of GraphLab Create **(1.8.3 or newer)**. Upgrade by\n",
    "```\n",
    "   pip install graphlab-create --upgrade\n",
    "```\n",
    "See [this page](https://dato.com/download/) for detailed instructions on upgrading."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "import graphlab\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Getting the data ready"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We will be using the same [LendingClub](https://www.lendingclub.com/) dataset as in the previous assignment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[INFO] GraphLab Create v1.8.3 started. Logging: C:\\Users\\linghao\\AppData\\Local\\Temp\\graphlab_server_1457411497.log.0\n"
     ]
    }
   ],
   "source": [
    "loans = graphlab.SFrame('lending-club-data.gl/')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Extracting the target and the feature columns\n",
    "\n",
    "We will now repeat some of the feature processing steps that we saw in the previous assignment:\n",
    "\n",
    "First, we re-assign the target to have +1 as a safe (good) loan, and -1 as a risky (bad) loan.\n",
    "\n",
    "Next, we select four categorical features: \n",
    "1. grade of the loan \n",
    "2. the length of the loan term\n",
    "3. the home ownership status: own, mortgage, rent\n",
    "4. number of years of employment."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "features = ['grade',              # grade of the loan\n",
    "            'term',               # the term of the loan\n",
    "            'home_ownership',     # home ownership status: own, mortgage or rent\n",
    "            'emp_length',         # number of years of employment\n",
    "           ]\n",
    "loans['safe_loans'] = loans['bad_loans'].apply(lambda x : +1 if x==0 else -1)\n",
    "loans.remove_column('bad_loans')\n",
    "target = 'safe_loans'\n",
    "loans = loans[features + [target]]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Subsample dataset to make sure classes are balanced"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Just as we did in the previous assignment, we will undersample the larger class (safe loans) in order to balance out our dataset. This means we are throwing away many data points. We use `seed=1` so everyone gets the same results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Percentage of safe loans                 : 0.502236174422\n",
      "Percentage of risky loans                : 0.497763825578\n",
      "Total number of loans in our new dataset : 46508\n"
     ]
    }
   ],
   "source": [
    "safe_loans_raw = loans[loans[target] == 1]\n",
    "risky_loans_raw = loans[loans[target] == -1]\n",
    "\n",
    "# Undersample the safe loans.\n",
    "percentage = len(risky_loans_raw)/float(len(safe_loans_raw))\n",
    "risky_loans = risky_loans_raw\n",
    "safe_loans = safe_loans_raw.sample(percentage, seed=1)\n",
    "loans_data = risky_loans_raw.append(safe_loans)\n",
    "\n",
    "print \"Percentage of safe loans                 :\", len(safe_loans) / float(len(loans_data))\n",
    "print \"Percentage of risky loans                :\", len(risky_loans) / float(len(loans_data))\n",
    "print \"Total number of loans in our new dataset :\", len(loans_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note:** There are many approaches for dealing with imbalanced data, including some where we modify the learning algorithm. These approaches are beyond the scope of this course, but some of them are reviewed in this [paper](http://ieeexplore.ieee.org/xpl/login.jsp?tp=&arnumber=5128907&url=http%3A%2F%2Fieeexplore.ieee.org%2Fiel5%2F69%2F5173046%2F05128907.pdf%3Farnumber%3D5128907 ). For this assignment, we use the simplest possible approach, where we subsample the overly represented class to get a more balanced dataset. In general, and especially when the data is highly imbalanced, we recommend using more advanced methods."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Transform categorical data into binary features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this assignment, we will work with **binary decision trees**. Since all of our features are currently categorical features, we want to turn them into binary features using 1-hot encoding. \n",
    "\n",
    "We can do so with the following code block (see the first assignments for more details):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "loans_data = risky_loans.append(safe_loans)\n",
    "for feature in features:\n",
    "    loans_data_one_hot_encoded = loans_data[feature].apply(lambda x: {x: 1})    \n",
    "    loans_data_unpacked = loans_data_one_hot_encoded.unpack(column_name_prefix=feature)\n",
    "    \n",
    "    # Change None's to 0's\n",
    "    for column in loans_data_unpacked.column_names():\n",
    "        loans_data_unpacked[column] = loans_data_unpacked[column].fillna(0)\n",
    "\n",
    "    loans_data.remove_column(feature)\n",
    "    loans_data.add_columns(loans_data_unpacked)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's see what the feature columns look like now:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['grade.A',\n",
       " 'grade.B',\n",
       " 'grade.C',\n",
       " 'grade.D',\n",
       " 'grade.E',\n",
       " 'grade.F',\n",
       " 'grade.G',\n",
       " 'term. 36 months',\n",
       " 'term. 60 months',\n",
       " 'home_ownership.MORTGAGE',\n",
       " 'home_ownership.OTHER',\n",
       " 'home_ownership.OWN',\n",
       " 'home_ownership.RENT',\n",
       " 'emp_length.1 year',\n",
       " 'emp_length.10+ years',\n",
       " 'emp_length.2 years',\n",
       " 'emp_length.3 years',\n",
       " 'emp_length.4 years',\n",
       " 'emp_length.5 years',\n",
       " 'emp_length.6 years',\n",
       " 'emp_length.7 years',\n",
       " 'emp_length.8 years',\n",
       " 'emp_length.9 years',\n",
       " 'emp_length.< 1 year',\n",
       " 'emp_length.n/a']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "features = loans_data.column_names()\n",
    "features.remove('safe_loans')  # Remove the response variable\n",
    "features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train-test split\n",
    "\n",
    "We split the data into training and test sets with 80% of the data in the training set and 20% of the data in the test set. We use `seed=1` so that everyone gets the same result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, test_data = loans_data.random_split(0.8, seed=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Weighted decision trees"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's modify our decision tree code from Module 5 to support weighting of individual data points."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Weighted error definition\n",
    "\n",
    "Consider a model with $N$ data points with:\n",
    "* Predictions $\\hat{y}_1 ... \\hat{y}_n$ \n",
    "* Target $y_1 ... y_n$ \n",
    "* Data point weights $\\alpha_1 ... \\alpha_n$.\n",
    "\n",
    "Then the **weighted error** is defined by:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "where $1[y_i \\neq \\hat{y_i}]$ is an indicator function that is set to $1$ if $y_i \\neq \\hat{y_i}$.\n",
    "\n",
    "\n",
    "### Write a function to compute weight of mistakes\n",
    "\n",
    "Write a function that calculates the weight of mistakes for making the \"weighted-majority\" predictions for a dataset. The function accepts two inputs:\n",
    "* `labels_in_node`: Targets $y_1 ... y_n$ \n",
    "* `data_weights`: Data point weights $\\alpha_1 ... \\alpha_n$\n",
    "\n",
    "We are interested in computing the (total) weight of mistakes, i.e.\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}].\n",
    "$$\n",
    "This quantity is analogous to the number of mistakes, except that each mistake now carries different weight. It is related to the weighted error in the following way:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$\n",
    "\n",
    "The function **intermediate_node_weighted_mistakes** should first compute two weights: \n",
    " * $\\mathrm{WM}_{-1}$: weight of mistakes when all predictions are $\\hat{y}_i = -1$ i.e $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{-1}$)\n",
    " * $\\mathrm{WM}_{+1}$: weight of mistakes when all predictions are $\\hat{y}_i = +1$ i.e $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{+1}$)\n",
    " \n",
    " where $\\mathbf{-1}$ and $\\mathbf{+1}$ are vectors where all values are -1 and +1 respectively.\n",
    " \n",
    "After computing $\\mathrm{WM}_{-1}$ and $\\mathrm{WM}_{+1}$, the function **intermediate_node_weighted_mistakes** should return the lower of the two weights of mistakes, along with the class associated with that weight. We have provided a skeleton for you with `YOUR CODE HERE` to be filled in several places."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def intermediate_node_weighted_mistakes(labels_in_node, data_weights):\n",
    "    # Sum the weights of all entries with label +1\n",
    "    total_weight_positive = sum(data_weights[labels_in_node == +1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all -1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_negative = total_weight_positive\n",
    "    \n",
    "    # Sum the weights of all entries with label -1\n",
    "    ### YOUR CODE HERE\n",
    "    total_weight_negative = sum(data_weights[labels_in_node == -1])\n",
    "    \n",
    "    # Weight of mistakes for predicting all +1's is equal to the sum above\n",
    "    ### YOUR CODE HERE\n",
    "    weighted_mistakes_all_positive = total_weight_negative\n",
    "    \n",
    "    # Return the tuple (weight, class_label) representing the lower of the two weights\n",
    "    #    class_label should be an integer of value +1 or -1.\n",
    "    # If the two weights are identical, return (weighted_mistakes_all_positive,+1)\n",
    "    ### YOUR CODE HERE\n",
    "    if weighted_mistakes_all_positive <= weighted_mistakes_all_negative:\n",
    "        return weighted_mistakes_all_positive, +1\n",
    "    else:\n",
    "        return weighted_mistakes_all_negative, -1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Test your **intermediate_node_weighted_mistakes** function, run the following cell:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_labels = graphlab.SArray([-1, -1, 1, 1, 1])\n",
    "example_data_weights = graphlab.SArray([1., 2., .5, 1., 1.])\n",
    "if intermediate_node_weighted_mistakes(example_labels, example_data_weights) == (2.5, -1):\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "**Quiz Question:** If we set the weights $\\mathbf{\\alpha} = 1$ for all data points, how is the weight of mistakes $\\mbox{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ related to the `classification error`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Function to pick best feature to split on"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We continue modifying our decision tree code from the earlier assignment to incorporate weighting of individual data points. The next step is to pick the best feature to split on.\n",
    "\n",
    "The **best_splitting_feature** function is similar to the one from the earlier assignment with two minor modifications:\n",
    "  1. The function **best_splitting_feature** should now accept an extra parameter `data_weights` to take account of weights of data points.\n",
    "  2. Instead of computing the number of mistakes in the left and right side of the split, we compute the weight of mistakes for both sides, add up the two weights, and divide it by the total weight of the data.\n",
    "  \n",
    "Complete the following function. Comments starting with `DIFFERENT HERE` mark the sections where the weighted version differs from the original implementation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# If the data is identical in each feature, this function should return None\n",
    "\n",
    "def best_splitting_feature(data, features, target, data_weights):\n",
    "    print data_weights\n",
    "    \n",
    "    # These variables will keep track of the best feature and the corresponding error\n",
    "    best_feature = None\n",
    "    best_error = float('+inf') \n",
    "    num_points = float(len(data))\n",
    "\n",
    "    # Loop through each feature to consider splitting on that feature\n",
    "    for feature in features:\n",
    "        \n",
    "        # The left split will have all data points where the feature value is 0\n",
    "        # The right split will have all data points where the feature value is 1\n",
    "        left_split = data[data[feature] == 0]\n",
    "        right_split = data[data[feature] == 1]\n",
    "        \n",
    "        # Apply the same filtering to data_weights to create left_data_weights, right_data_weights\n",
    "        ## YOUR CODE HERE\n",
    "        left_data_weights = data_weights[data[feature] == 0]\n",
    "        right_data_weights = data_weights[data[feature] == 1]\n",
    "            \n",
    "        # DIFFERENT HERE\n",
    "        # Calculate the weight of mistakes for left and right sides\n",
    "        ## YOUR CODE HERE\n",
    "        left_weighted_mistakes, left_class = intermediate_node_weighted_mistakes(left_split[target], left_data_weights)\n",
    "        right_weighted_mistakes, right_class = intermediate_node_weighted_mistakes(right_split[target], right_data_weights)\n",
    "        \n",
    "        # DIFFERENT HERE\n",
    "        # Compute weighted error by computing\n",
    "        #  ( [weight of mistakes (left)] + [weight of mistakes (right)] ) / [total weight of all data points]\n",
    "        ## YOUR CODE HERE\n",
    "        error = (left_weighted_mistakes + right_weighted_mistakes) / (sum(left_data_weights) + sum(right_data_weights))\n",
    "        \n",
    "        # If this is the best error we have found so far, store the feature and the error\n",
    "        if error < best_error:\n",
    "            best_feature = feature\n",
    "            best_error = error\n",
    "    \n",
    "    # Return the best feature we found\n",
    "    return best_feature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Checkpoint:** Now, we have another checkpoint to make sure you are on the right track."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, 1.5, ... ]\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray(len(train_data)* [1.5])\n",
    "if best_splitting_feature(train_data, features, target, example_data_weights) == 'term. 36 months':\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Note**. If you get an exception in the line of \"the logical filter has different size than the array\", try upgradting your GraphLab Create installation to 1.8.3 or newer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Very Optional**. Relationship between weighted error and weight of mistakes\n",
    "\n",
    "By definition, the weighted error is the weight of mistakes divided by the weight of all data points, so\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}}) = \\frac{\\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]}{\\sum_{i=1}^{n} \\alpha_i} = \\frac{\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\sum_{i=1}^{n} \\alpha_i}.\n",
    "$$\n",
    "\n",
    "In the code above, we obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ from the two weights of mistakes from both sides, $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}})$ and $\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})$. First, notice that the overall weight of mistakes $\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$ can be broken into two weights of mistakes over either side of the split:\n",
    "$$\n",
    "\\mathrm{WM}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\sum_{i=1}^{n} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    "= \\sum_{\\mathrm{left}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\n",
    " + \\sum_{\\mathrm{right}} \\alpha_i \\times 1[y_i \\neq \\hat{y_i}]\\\\\n",
    "= \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})\n",
    "$$\n",
    "We then divide through by the total weight of all data points to obtain $\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})$:\n",
    "$$\n",
    "\\mathrm{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})\n",
    "= \\frac{\\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{left}}, \\mathbf{\\hat{y}}_{\\mathrm{left}}) + \\mathrm{WM}(\\mathbf{\\alpha}_{\\mathrm{right}}, \\mathbf{\\hat{y}}_{\\mathrm{right}})}{\\sum_{i=1}^{n} \\alpha_i}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Building the tree\n",
    "\n",
    "With the above functions implemented correctly, we are now ready to build our decision tree. Recall from the previous assignments that each node in the decision tree is represented as a dictionary which contains the following keys:\n",
    "\n",
    "    { \n",
    "       'is_leaf'            : True/False.\n",
    "       'prediction'         : Prediction at the leaf node.\n",
    "       'left'               : (dictionary corresponding to the left tree).\n",
    "       'right'              : (dictionary corresponding to the right tree).\n",
    "       'features_remaining' : List of features that are posible splits.\n",
    "    }\n",
    "    \n",
    "Let us start with a function that creates a leaf node given a set of target values:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def create_leaf(target_values, data_weights):\n",
    "    \n",
    "    # Create a leaf node\n",
    "    leaf = {'splitting_feature' : None,\n",
    "            'is_leaf': True}\n",
    "    \n",
    "    # Computed weight of mistakes.\n",
    "    weighted_error, best_class = intermediate_node_weighted_mistakes(target_values, data_weights)\n",
    "    # Store the predicted class (1 or -1) in leaf['prediction']\n",
    "    leaf['prediction'] = best_class ## YOUR CODE HERE\n",
    "    \n",
    "    return leaf "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We provide a function that learns a weighted decision tree recursively and implements 3 stopping conditions:\n",
    "1. All data points in a node are from the same class.\n",
    "2. No more features to split on.\n",
    "3. Stop growing the tree when the tree depth reaches **max_depth**. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def weighted_decision_tree_create(data, features, target, data_weights, current_depth = 1, max_depth = 10):\n",
    "    remaining_features = features[:] # Make a copy of the features.\n",
    "    target_values = data[target]\n",
    "    print \"--------------------------------------------------------------------\"\n",
    "    print \"Subtree, depth = %s (%s data points).\" % (current_depth, len(target_values))\n",
    "    \n",
    "    # Stopping condition 1. Error is 0.\n",
    "    if intermediate_node_weighted_mistakes(target_values, data_weights)[0] <= 1e-15:\n",
    "        print \"Stopping condition 1 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # Stopping condition 2. No more features.\n",
    "    if remaining_features == []:\n",
    "        print \"Stopping condition 2 reached.\"                \n",
    "        return create_leaf(target_values, data_weights)    \n",
    "    \n",
    "    # Additional stopping condition (limit tree depth)\n",
    "    if current_depth > max_depth:\n",
    "        print \"Reached maximum depth. Stopping for now.\"\n",
    "        return create_leaf(target_values, data_weights)\n",
    "    \n",
    "    # If all the datapoints are the same, splitting_feature will be None. Create a leaf\n",
    "    splitting_feature = best_splitting_feature(data, features, target, data_weights)\n",
    "    remaining_features.remove(splitting_feature)\n",
    "        \n",
    "    left_split = data[data[splitting_feature] == 0]\n",
    "    right_split = data[data[splitting_feature] == 1]\n",
    "    \n",
    "    left_data_weights = data_weights[data[splitting_feature] == 0]\n",
    "    right_data_weights = data_weights[data[splitting_feature] == 1]\n",
    "    \n",
    "    print \"Split on feature %s. (%s, %s)\" % (\\\n",
    "              splitting_feature, len(left_split), len(right_split))\n",
    "    \n",
    "    # Create a leaf node if the split is \"perfect\"\n",
    "    if len(left_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(left_split[target], data_weights)\n",
    "    if len(right_split) == len(data):\n",
    "        print \"Creating leaf node.\"\n",
    "        return create_leaf(right_split[target], data_weights)\n",
    "    \n",
    "    # Repeat (recurse) on left and right subtrees\n",
    "    left_tree = weighted_decision_tree_create(\n",
    "        left_split, remaining_features, target, left_data_weights, current_depth + 1, max_depth)\n",
    "    right_tree = weighted_decision_tree_create(\n",
    "        right_split, remaining_features, target, right_data_weights, current_depth + 1, max_depth)\n",
    "    \n",
    "    return {'is_leaf'          : False, \n",
    "            'prediction'       : None,\n",
    "            'splitting_feature': splitting_feature,\n",
    "            'left'             : left_tree, \n",
    "            'right'            : right_tree}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is a recursive function to count the nodes in your tree:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def count_nodes(tree):\n",
    "    if tree['is_leaf']:\n",
    "        return 1\n",
    "    return 1 + count_nodes(tree['left']) + count_nodes(tree['right'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Run the following test code to check your implementation. Make sure you get **'Test passed'** before proceeding."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature grade.A. (9122, 101)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (9122 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (101 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature grade.D. (23300, 4701)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (23300 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (4701 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "Test passed!\n"
     ]
    }
   ],
   "source": [
    "example_data_weights = graphlab.SArray([1.0 for i in range(len(train_data))])\n",
    "small_data_decision_tree = weighted_decision_tree_create(train_data, features, target,\n",
    "                                        example_data_weights, max_depth=2)\n",
    "if count_nodes(small_data_decision_tree) == 7:\n",
    "    print 'Test passed!'\n",
    "else:\n",
    "    print 'Test failed... try again!'\n",
    "    print 'Number of nodes found:', count_nodes(small_data_decision_tree)\n",
    "    print 'Number of nodes that should be there: 7' "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us take a quick look at what the trained tree is like. You should get something that looks like the following\n",
    "\n",
    "```\n",
    "{'is_leaf': False,\n",
    "    'left': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.A'\n",
    "     },\n",
    "    'prediction': None,\n",
    "    'right': {'is_leaf': False,\n",
    "        'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
    "        'prediction': None,\n",
    "        'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
    "        'splitting_feature': 'grade.D'\n",
    "     },\n",
    "     'splitting_feature': 'term. 36 months'\n",
    "}```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'is_leaf': False,\n",
       " 'left': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.A'},\n",
       " 'prediction': None,\n",
       " 'right': {'is_leaf': False,\n",
       "  'left': {'is_leaf': True, 'prediction': 1, 'splitting_feature': None},\n",
       "  'prediction': None,\n",
       "  'right': {'is_leaf': True, 'prediction': -1, 'splitting_feature': None},\n",
       "  'splitting_feature': 'grade.D'},\n",
       " 'splitting_feature': 'term. 36 months'}"
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "small_data_decision_tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Making predictions with a weighted decision tree"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We give you a function that classifies one data point. It can also return the probability if you want to play around with that as well."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def classify(tree, x, annotate = False):   \n",
    "    # If the node is a leaf node.\n",
    "    if tree['is_leaf']:\n",
    "        if annotate: \n",
    "            print \"At leaf, predicting %s\" % tree['prediction']\n",
    "        return tree['prediction'] \n",
    "    else:\n",
    "        # Split on feature.\n",
    "        split_feature_value = x[tree['splitting_feature']]\n",
    "        if annotate: \n",
    "            print \"Split on %s = %s\" % (tree['splitting_feature'], split_feature_value)\n",
    "        if split_feature_value == 0:\n",
    "            return classify(tree['left'], x, annotate)\n",
    "        else:\n",
    "            return classify(tree['right'], x, annotate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Evaluating the tree\n",
    "\n",
    "Now, we will write a function to evaluate a decision tree by computing the classification error of the tree on the given dataset.\n",
    "\n",
    "Again, recall that the **classification error** is defined as follows:\n",
    "$$\n",
    "\\mbox{classification error} = \\frac{\\mbox{# mistakes}}{\\mbox{# all data points}}\n",
    "$$\n",
    "\n",
    "The function called **evaluate_classification_error** takes in as input:\n",
    "1. `tree` (as described above)\n",
    "2. `data` (an SFrame)\n",
    "\n",
    "The function does not change because of adding data point weights."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def evaluate_classification_error(tree, data):\n",
    "    # Apply the classify(tree, x) to each row in your data\n",
    "    prediction = data.apply(lambda x: classify(tree, x))\n",
    "    \n",
    "    # Once you've made the predictions, calculate the classification error\n",
    "    return (prediction != data[target]).sum() / float(len(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.3981042654028436"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree, test_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example: Training a weighted decision tree\n",
    "\n",
    "To build intuition on how weighted data points affect the tree being built, consider the following:\n",
    "\n",
    "Suppose we only care about making good predictions for the **first 10 and last 10 items** in `train_data`, we assign weights:\n",
    "* 1 to the last 10 items \n",
    "* 1 to the first 10 items \n",
    "* and 0 to the rest. \n",
    "\n",
    "Let us fit a weighted decision tree with `max_depth = 2`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\n",
      "Split on feature home_ownership.RENT. (20514, 16710)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (20514 data points).\n",
      "[1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\n",
      "Split on feature grade.F. (19613, 901)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (19613 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (901 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (16710 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, 0.0, ... ]\n",
      "Split on feature grade.D. (13315, 3395)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (13315 data points).\n",
      "Stopping condition 1 reached.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 3 (3395 data points).\n",
      "Stopping condition 1 reached.\n"
     ]
    }
   ],
   "source": [
    "# Assign weights\n",
    "example_data_weights = graphlab.SArray([1.] * 10 + [0.]*(len(train_data) - 20) + [1.] * 10)\n",
    "\n",
    "# Train a weighted decision tree model.\n",
    "small_data_decision_tree_subset_20 = weighted_decision_tree_create(train_data, features, target,\n",
    "                         example_data_weights, max_depth=2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we will compute the classification error on the `subset_20`, i.e. the subset of data points whose weight is 1 (namely the first and last 10 data points)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.05"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "subset_20 = train_data.head(10).append(train_data.tail(10))\n",
    "evaluate_classification_error(small_data_decision_tree_subset_20, subset_20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us compare the classification error of the model `small_data_decision_tree_subset_20` on the entire test set `train_data`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.48124865678057166"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "evaluate_classification_error(small_data_decision_tree_subset_20, train_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The model `small_data_decision_tree_subset_20` performs **a lot** better on `subset_20` than on `train_data`.\n",
    "\n",
    "So, what does this mean?\n",
    "* The points with higher weights are the ones that are more important during the training process of the weighted decision tree.\n",
    "* The points with zero weights are basically ignored during training.\n",
    "\n",
    "**Quiz Question**: Will you get the same model as `small_data_decision_tree_subset_20` if you trained a decision tree with only the 20 data points with non-zero weights from the set of points in `subset_20`?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Implementing your own Adaboost (on decision stumps)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have a weighted decision tree working, it takes only a bit of work to implement Adaboost. For the sake of simplicity, let us stick with **decision tree stumps** by training trees with **`max_depth=1`**."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Recall from the lecture the procedure for Adaboost:\n",
    "\n",
    "1\\. Start with unweighted data with $\\alpha_j = 1$\n",
    "\n",
    "2\\. For t = 1,...T:\n",
    "  * Learn $f_t(x)$ with data weights $\\alpha_j$\n",
    "  * Compute coefficient $\\hat{w}_t$:\n",
    "     $$\\hat{w}_t = \\frac{1}{2}\\ln{\\left(\\frac{1- \\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}{\\mbox{E}(\\mathbf{\\alpha}, \\mathbf{\\hat{y}})}\\right)}$$\n",
    "  * Re-compute weights $\\alpha_j$:\n",
    "     $$\\alpha_j \\gets \\begin{cases}\n",
    "     \\alpha_j \\exp{(-\\hat{w}_t)} & \\text{ if }f_t(x_j) = y_j\\\\\n",
    "     \\alpha_j \\exp{(\\hat{w}_t)} & \\text{ if }f_t(x_j) \\neq y_j\n",
    "     \\end{cases}$$\n",
    "  * Normalize weights $\\alpha_j$:\n",
    "      $$\\alpha_j \\gets \\frac{\\alpha_j}{\\sum_{i=1}^{N}{\\alpha_i}} $$\n",
    "  \n",
    "Complete the skeleton for the following code to implement **adaboost_with_tree_stumps**. Fill in the places with `YOUR CODE HERE`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "from math import log\n",
    "from math import exp\n",
    "\n",
    "def adaboost_with_tree_stumps(data, features, target, num_tree_stumps):\n",
    "    # start with unweighted data\n",
    "    alpha = graphlab.SArray([1.]*len(data))\n",
    "    weights = []\n",
    "    tree_stumps = []\n",
    "    target_values = data[target]\n",
    "    \n",
    "    for t in xrange(num_tree_stumps):\n",
    "        print '====================================================='\n",
    "        print 'Adaboost Iteration %d' % t\n",
    "        print '====================================================='        \n",
    "        # Learn a weighted decision tree stump. Use max_depth=1\n",
    "        tree_stump = weighted_decision_tree_create(data, features, target, data_weights=alpha, max_depth=1)\n",
    "        tree_stumps.append(tree_stump)\n",
    "        \n",
    "        # Make predictions\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Produce a Boolean array indicating whether\n",
    "        # each data point was correctly classified\n",
    "        is_correct = predictions == target_values\n",
    "        is_wrong   = predictions != target_values\n",
    "        \n",
    "        # Compute weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weighted_error = sum(alpha * is_wrong) / sum(alpha)\n",
    "        \n",
    "        # Compute model coefficient using weighted error\n",
    "        # YOUR CODE HERE\n",
    "        weight = .5 * log((1 - weighted_error) / weighted_error)\n",
    "        weights.append(weight)\n",
    "        \n",
    "        # Adjust weights on data point\n",
    "        adjustment = is_correct.apply(lambda is_correct : exp(-weight) if is_correct else exp(weight))\n",
    "        \n",
    "        # Scale alpha by multiplying by adjustment \n",
    "        # Then normalize data points weights\n",
    "        ## YOUR CODE HERE \n",
    "        alpha *= adjustment\n",
    "        alpha /= sum(alpha)\n",
    "    \n",
    "    return weights, tree_stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking your Adaboost code\n",
    "\n",
    "Train an ensemble of **two** tree stumps and see which features those stumps split on. We will run the algorithm with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 2`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, target, num_tree_stumps=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def print_stump(tree):\n",
    "    split_name = tree['splitting_feature'] # split_name is something like 'term. 36 months'\n",
    "    if split_name is None:\n",
    "        print \"(leaf, label: %s)\" % tree['prediction']\n",
    "        return None\n",
    "    split_feature, split_value = split_name.split('.')\n",
    "    print '                       root'\n",
    "    print '         |---------------|----------------|'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '  [{0} == 0]{1}[{0} == 1]    '.format(split_name, ' '*(27-len(split_name)))\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '         |                                |'\n",
    "    print '    (%s)                 (%s)' \\\n",
    "        % (('leaf, label: ' + str(tree['left']['prediction']) if tree['left']['is_leaf'] else 'subtree'),\n",
    "           ('leaf, label: ' + str(tree['right']['prediction']) if tree['right']['is_leaf'] else 'subtree'))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the first stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [term. 36 months == 0]            [term. 36 months == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here is what the next stump looks like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                       root\n",
      "         |---------------|----------------|\n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "  [grade.A == 0]                    [grade.A == 1]    \n",
      "         |                                |\n",
      "         |                                |\n",
      "         |                                |\n",
      "    (leaf, label: -1)                 (leaf, label: 1)\n"
     ]
    }
   ],
   "source": [
    "print_stump(tree_stumps[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.15802933659263743, 0.1768236329364191]\n"
     ]
    }
   ],
   "source": [
    "print stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "If your Adaboost is correctly implemented, the following things should be true:\n",
    "\n",
    "* `tree_stumps[0]` should split on **term. 36 months** with the prediction -1 on the left and +1 on the right.\n",
    "* `tree_stumps[1]` should split on **grade.A** with the prediction -1 on the left and +1 on the right.\n",
    "* Weights should be approximately `[0.158, 0.177]` \n",
    "\n",
    "**Reminders**\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) and data point weights ($\\mathbf{\\alpha}$) are two different concepts.\n",
    "- Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble.\n",
    "- Data point weights ($\\mathbf{\\alpha}$) tell you how important each data point is while training a decision stump."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training a boosted ensemble of 10 stumps"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let us train an ensemble of 10 decision tree stumps with Adaboost. We run the **adaboost_with_tree_stumps** function with the following parameters:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 10`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.1788543607076036e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, ... ]\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0310733650963335e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.3126297532772446e-05, 1.9505362517888515e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 1.9505362517888515e-05, 2.3498279556392647e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 3.2232842342757394e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 3.2232842342757394e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 1.685945653661868e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.3498279556392647e-05, 1.9505362517888515e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9036029491260022e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 3.454614596431647e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 1.580135495610065e-05, 2.1674888235099766e-05, 1.8281203550253472e-05, 2.9859954966688172e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 3.454614596431647e-05, 2.611192602213042e-05, 3.454614596431647e-05, 1.8281203550253472e-05, 2.518471647126885e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.718998153944582e-05, 2.1768405090908584e-05, 1.580135495610065e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 4.302653946080036e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 3.454614596431647e-05, 2.1768405090908584e-05, 2.9859954966688172e-05, 2.711211669197836e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.611192602213042e-05, 2.1768405090908584e-05, 2.1674888235099766e-05, 3.454614596431647e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.711211669197836e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 4.302653946080036e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.1674888235099766e-05, 1.580135495610065e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 4.302653946080036e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.518471647126885e-05, 1.8281203550253472e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, 2.5076523175113537e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.0209905164836932e-05, 2.611192602213042e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, ... ]\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0348064240517002e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 3.692719627624719e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 1.6890443770407573e-05, 2.3168803054036615e-05, 1.954121285666819e-05, 3.1918015369753364e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 3.692719627624719e-05, 2.791165817356422e-05, 3.692719627624719e-05, 1.954121285666819e-05, 2.692054185311439e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.9753254942987325e-05, 2.3268765443275537e-05, 1.6890443770407573e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 4.5992090388254305e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 3.692719627624719e-05, 2.3268765443275537e-05, 3.1918015369753364e-05, 2.8980785746211437e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 2.791165817356422e-05, 2.3268765443275537e-05, 2.3168803054036615e-05, 3.692719627624719e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.8980785746211437e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 4.5992090388254305e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 2.3168803054036615e-05, 1.6890443770407573e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 4.5992090388254305e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 2.692054185311439e-05, 1.954121285666819e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 3.1918015369753364e-05, 2.680489146806049e-05, 2.0348064240517002e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.2292085451761906e-05, 2.791165817356422e-05, 2.680489146806049e-05, 3.1918015369753364e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.929629127644232e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 1.601739206883269e-05, 2.1971228661987836e-05, 1.853114530798475e-05, 3.0268202137594344e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 1.853114530798475e-05, 2.5529043489207745e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 4.2044978849681946e-05, 2.2066024086095184e-05, 1.601739206883269e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.8643475116694395e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 3.501846303148477e-05, 2.2066024086095184e-05, 3.0268202137594344e-05, 3.0651490689105764e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 2.6468929907010296e-05, 2.2066024086095184e-05, 2.1971228661987836e-05, 3.501846303148477e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 3.0651490689105764e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 4.8643475116694395e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 2.1971228661987836e-05, 1.601739206883269e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 4.8643475116694395e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 2.5529043489207745e-05, 1.853114530798475e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, 2.5419370967544032e-05, 1.929629127644232e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 3.062293687672824e-05, 2.6468929907010296e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0173505904006534e-05, 1.8492186308167507e-05, 2.3069151517330548e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 1.674554705037387e-05, 2.2970046667569232e-05, 1.937357619259354e-05, 3.1644202804498845e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 1.937357619259354e-05, 2.668960072042015e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 4.3956354975497195e-05, 2.3069151517330548e-05, 1.674554705037387e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 5.085482066992069e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 3.6610411845168866e-05, 2.3069151517330548e-05, 3.1644202804498845e-05, 3.204491575737045e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 2.7672214629331413e-05, 2.3069151517330548e-05, 2.2970046667569232e-05, 3.6610411845168866e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 3.204491575737045e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 5.085482066992069e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 2.2970046667569232e-05, 1.674554705037387e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 5.085482066992069e-05, 5.085482066992069e-05, 2.65749424562183e-05, 2.668960072042015e-05, 1.937357619259354e-05, 5.085482066992069e-05, 2.65749424562183e-05, 3.1644202804498845e-05, 2.65749424562183e-05, 1.8492186308167507e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 3.201506388094883e-05, 2.7672214629331413e-05, 2.65749424562183e-05, 3.1644202804498845e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9605331743486973e-05, 1.7971365461170967e-05, 2.2419423311453615e-05, 2.689284403348397e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 2.689284403348397e-05, 3.557930252211e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 1.6273919204274127e-05, 2.2323109687723337e-05, 1.882793155145546e-05, 3.075296364907836e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 3.111337965555469e-05, 4.526825592387652e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 3.111337965555469e-05, 4.526825592387652e-05, 2.689284403348397e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 3.557930252211e-05, 2.689284403348397e-05, 3.557930252211e-05, 1.882793155145546e-05, 2.593790482997429e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 4.526825592387652e-05, 2.2419423311453615e-05, 1.6273919204274127e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 5.237261002037342e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 3.557930252211e-05, 2.2419423311453615e-05, 3.075296364907836e-05, 3.3001313424926466e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 2.689284403348397e-05, 2.2419423311453615e-05, 2.2323109687723337e-05, 3.557930252211e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 4.526825592387652e-05, 2.689284403348397e-05, 3.3001313424926466e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 5.237261002037342e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 2.2323109687723337e-05, 1.6273919204274127e-05, 3.075296364907836e-05, 2.689284403348397e-05, 5.237261002037342e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 2.593790482997429e-05, 1.882793155145546e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 3.075296364907836e-05, 2.5826475843981168e-05, 1.7971365461170967e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 3.111337965555469e-05, 2.689284403348397e-05, 2.5826475843981168e-05, 3.075296364907836e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, features, \n",
    "                                target, num_tree_stumps=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Making predictions\n",
    "\n",
    "Recall from the lecture that in order to make predictions, we use the following formula:\n",
    "$$\n",
    "\\hat{y} = sign\\left(\\sum_{t=1}^T \\hat{w}_t f_t(x)\\right)\n",
    "$$\n",
    "\n",
    "We need to do the following things:\n",
    "- Compute the predictions $f_t(x)$ using the $t$-th decision tree\n",
    "- Compute $\\hat{w}_t f_t(x)$ by multiplying the `stump_weights` with the predictions $f_t(x)$ from the decision trees\n",
    "- Sum the weighted predictions over each stump in the ensemble.\n",
    "\n",
    "Complete the following skeleton for making predictions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "def predict_adaboost(stump_weights, tree_stumps, data):\n",
    "    scores = graphlab.SArray([0.]*len(data))\n",
    "    \n",
    "    for i, tree_stump in enumerate(tree_stumps):\n",
    "        predictions = data.apply(lambda x: classify(tree_stump, x))\n",
    "        \n",
    "        # Accumulate predictions on scaores array\n",
    "        # YOUR CODE HERE\n",
    "        scores += stump_weights[i] * predictions\n",
    "        \n",
    "    return scores.apply(lambda score : +1 if score > 0 else -1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy of 10-component ensemble = 0.620314519604\n"
     ]
    }
   ],
   "source": [
    "predictions = predict_adaboost(stump_weights, tree_stumps, test_data)\n",
    "accuracy = graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "print 'Accuracy of 10-component ensemble = %s' % accuracy "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let us take a quick look what the `stump_weights` look like at the end of each iteration of the 10-stump ensemble:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[0.15802933659263743,\n",
       " 0.1768236329364191,\n",
       " 0.09311888971129693,\n",
       " 0.07288885525840554,\n",
       " 0.06706306914118143,\n",
       " 0.06456916961644447,\n",
       " 0.05456055779178564,\n",
       " 0.04351093673362621,\n",
       " 0.02898871150041245,\n",
       " 0.02596250969152032]"
      ]
     },
     "execution_count": 61,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "stump_weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question:** Are the weights monotonically decreasing, monotonically increasing, or neither?\n",
    "\n",
    "**Reminder**: Stump weights ($\\mathbf{\\hat{w}}$) tell you how important each stump is while making predictions with the entire boosted ensemble."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Performance plots\n",
    "\n",
    "In this section, we will try to reproduce some of the performance plots dicussed in the lecture.\n",
    "\n",
    "### How does accuracy change with adding stumps to the ensemble?\n",
    "\n",
    "We will now train an ensemble with:\n",
    "* `train_data`\n",
    "* `features`\n",
    "* `target`\n",
    "* `num_tree_stumps = 30`\n",
    "\n",
    "Once we are done with this, we will then do the following:\n",
    "* Compute the classification error at the end of each iteration.\n",
    "* Plot a curve of classification error vs iteration.\n",
    "\n",
    "First, lets train the model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=====================================================\n",
      "Adaboost Iteration 0\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 1\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 2.3224487900035514e-05, 2.3224487900035514e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, 3.1857279388331606e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 2\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.815101392687598e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 3.861504803005498e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 3.861504803005498e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 1.9765462704704058e-05, 1.9765462704704058e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, 2.7112497392135943e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 3\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.1788543607076036e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.4808968052177085e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 3.103239239540615e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 4.2567465809538e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.4808968052177085e-05, 1.8086151404102345e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 4.2567465809538e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 1.8086151404102345e-05, 4.2567465809538e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.1788543607076036e-05, 2.1788543607076036e-05, 2.9887579185520227e-05, 2.9887579185520227e-05, 2.4808968052177085e-05, 2.9887579185520227e-05, ... ]\n",
      "Split on feature home_ownership.MORTGAGE. (19846, 17378)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (19846 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (17378 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 4\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0310733650963335e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.3126297532772446e-05, 1.9505362517888515e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 3.2232842342757394e-05, 1.9505362517888515e-05, 2.3498279556392647e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.968032356110462e-05, 2.0310733650963335e-05, 1.685945653661868e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 3.968032356110462e-05, 3.2232842342757394e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 3.2232842342757394e-05, 2.3126297532772446e-05, 2.3126297532772446e-05, 3.968032356110462e-05, 2.7860451403095862e-05, 2.8927617552674857e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 4.5907713229390207e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.3126297532772446e-05, 1.685945653661868e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 4.5907713229390207e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.3498279556392647e-05, 1.9505362517888515e-05, 4.5907713229390207e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.7860451403095862e-05, 2.7860451403095862e-05, 2.0310733650963335e-05, 2.0310733650963335e-05, 3.2232842342757394e-05, 2.7860451403095862e-05, 2.6755715173470646e-05, 2.7860451403095862e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 5\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9036029491260022e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 3.454614596431647e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 1.580135495610065e-05, 2.1674888235099766e-05, 1.8281203550253472e-05, 2.9859954966688172e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 3.0209905164836932e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 2.9859954966688172e-05, 3.454614596431647e-05, 2.611192602213042e-05, 3.454614596431647e-05, 1.8281203550253472e-05, 2.518471647126885e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.718998153944582e-05, 2.1768405090908584e-05, 1.580135495610065e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 4.302653946080036e-05, 2.9859954966688172e-05, 2.9859954966688172e-05, 3.718998153944582e-05, 3.454614596431647e-05, 2.1768405090908584e-05, 2.9859954966688172e-05, 2.711211669197836e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.611192602213042e-05, 2.1768405090908584e-05, 2.1674888235099766e-05, 3.454614596431647e-05, 2.1674888235099766e-05, 2.1674888235099766e-05, 3.718998153944582e-05, 2.611192602213042e-05, 2.711211669197836e-05, 1.9036029491260022e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 4.302653946080036e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.1674888235099766e-05, 1.580135495610065e-05, 2.9859954966688172e-05, 2.611192602213042e-05, 4.302653946080036e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.518471647126885e-05, 1.8281203550253472e-05, 4.302653946080036e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, 2.5076523175113537e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 1.9036029491260022e-05, 2.611192602213042e-05, 2.9859954966688172e-05, 1.9036029491260022e-05, 2.1768405090908584e-05, 3.0209905164836932e-05, 2.611192602213042e-05, 2.5076523175113537e-05, 2.9859954966688172e-05, ... ]\n",
      "Split on feature grade.E. (33815, 3409)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33815 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3409 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 6\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0348064240517002e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 3.692719627624719e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 1.6890443770407573e-05, 2.3168803054036615e-05, 1.954121285666819e-05, 3.1918015369753364e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 3.2292085451761906e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.1918015369753364e-05, 3.692719627624719e-05, 2.791165817356422e-05, 3.692719627624719e-05, 1.954121285666819e-05, 2.692054185311439e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.9753254942987325e-05, 2.3268765443275537e-05, 1.6890443770407573e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 4.5992090388254305e-05, 3.1918015369753364e-05, 3.1918015369753364e-05, 3.9753254942987325e-05, 3.692719627624719e-05, 2.3268765443275537e-05, 3.1918015369753364e-05, 2.8980785746211437e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 2.791165817356422e-05, 2.3268765443275537e-05, 2.3168803054036615e-05, 3.692719627624719e-05, 2.3168803054036615e-05, 2.3168803054036615e-05, 3.9753254942987325e-05, 2.791165817356422e-05, 2.8980785746211437e-05, 1.7882944632693728e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 1.7882944632693728e-05, 4.5992090388254305e-05, 2.791165817356422e-05, 1.7882944632693728e-05, 2.3168803054036615e-05, 1.6890443770407573e-05, 3.1918015369753364e-05, 2.791165817356422e-05, 4.5992090388254305e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 2.692054185311439e-05, 1.954121285666819e-05, 4.5992090388254305e-05, 2.680489146806049e-05, 3.1918015369753364e-05, 2.680489146806049e-05, 2.0348064240517002e-05, 2.453022714222767e-05, 2.0348064240517002e-05, 2.791165817356422e-05, 3.1918015369753364e-05, 2.0348064240517002e-05, 2.3268765443275537e-05, 3.2292085451761906e-05, 2.791165817356422e-05, 2.680489146806049e-05, 3.1918015369753364e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 7\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.929629127644232e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 1.601739206883269e-05, 2.1971228661987836e-05, 1.853114530798475e-05, 3.0268202137594344e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 3.062293687672824e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 3.0268202137594344e-05, 3.501846303148477e-05, 2.6468929907010296e-05, 3.501846303148477e-05, 1.853114530798475e-05, 2.5529043489207745e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 4.2044978849681946e-05, 2.2066024086095184e-05, 1.601739206883269e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.8643475116694395e-05, 3.0268202137594344e-05, 3.0268202137594344e-05, 4.2044978849681946e-05, 3.501846303148477e-05, 2.2066024086095184e-05, 3.0268202137594344e-05, 3.0651490689105764e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 2.6468929907010296e-05, 2.2066024086095184e-05, 2.1971228661987836e-05, 3.501846303148477e-05, 2.1971228661987836e-05, 2.1971228661987836e-05, 4.2044978849681946e-05, 2.6468929907010296e-05, 3.0651490689105764e-05, 1.6958591462761244e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.6958591462761244e-05, 4.8643475116694395e-05, 2.6468929907010296e-05, 1.6958591462761244e-05, 2.1971228661987836e-05, 1.601739206883269e-05, 3.0268202137594344e-05, 2.6468929907010296e-05, 4.8643475116694395e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 2.5529043489207745e-05, 1.853114530798475e-05, 4.8643475116694395e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, 2.5419370967544032e-05, 1.929629127644232e-05, 2.3262281975265167e-05, 1.929629127644232e-05, 2.6468929907010296e-05, 3.0268202137594344e-05, 1.929629127644232e-05, 2.2066024086095184e-05, 3.062293687672824e-05, 2.6468929907010296e-05, 2.5419370967544032e-05, 3.0268202137594344e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 8\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0173505904006534e-05, 1.8492186308167507e-05, 2.3069151517330548e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 1.674554705037387e-05, 2.2970046667569232e-05, 1.937357619259354e-05, 3.1644202804498845e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 3.201506388094883e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 3.1644202804498845e-05, 3.6610411845168866e-05, 2.7672214629331413e-05, 3.6610411845168866e-05, 1.937357619259354e-05, 2.668960072042015e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 4.3956354975497195e-05, 2.3069151517330548e-05, 1.674554705037387e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 5.085482066992069e-05, 3.1644202804498845e-05, 3.1644202804498845e-05, 4.3956354975497195e-05, 3.6610411845168866e-05, 2.3069151517330548e-05, 3.1644202804498845e-05, 3.204491575737045e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 2.7672214629331413e-05, 2.3069151517330548e-05, 2.2970046667569232e-05, 3.6610411845168866e-05, 2.2970046667569232e-05, 2.2970046667569232e-05, 4.3956354975497195e-05, 2.7672214629331413e-05, 3.204491575737045e-05, 1.7729533623661425e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 1.7729533623661425e-05, 5.085482066992069e-05, 2.7672214629331413e-05, 1.7729533623661425e-05, 2.2970046667569232e-05, 1.674554705037387e-05, 3.1644202804498845e-05, 2.7672214629331413e-05, 5.085482066992069e-05, 5.085482066992069e-05, 2.65749424562183e-05, 2.668960072042015e-05, 1.937357619259354e-05, 5.085482066992069e-05, 2.65749424562183e-05, 3.1644202804498845e-05, 2.65749424562183e-05, 1.8492186308167507e-05, 2.431979161413232e-05, 2.0173505904006534e-05, 2.7672214629331413e-05, 3.1644202804498845e-05, 2.0173505904006534e-05, 2.3069151517330548e-05, 3.201506388094883e-05, 2.7672214629331413e-05, 2.65749424562183e-05, 3.1644202804498845e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 9\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9605331743486973e-05, 1.7971365461170967e-05, 2.2419423311453615e-05, 2.689284403348397e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 2.689284403348397e-05, 3.557930252211e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 1.6273919204274127e-05, 2.2323109687723337e-05, 1.882793155145546e-05, 3.075296364907836e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 3.111337965555469e-05, 4.526825592387652e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 3.111337965555469e-05, 4.526825592387652e-05, 2.689284403348397e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 3.075296364907836e-05, 3.557930252211e-05, 2.689284403348397e-05, 3.557930252211e-05, 1.882793155145546e-05, 2.593790482997429e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 4.526825592387652e-05, 2.2419423311453615e-05, 1.6273919204274127e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 3.075296364907836e-05, 3.075296364907836e-05, 3.075296364907836e-05, 5.237261002037342e-05, 3.075296364907836e-05, 3.075296364907836e-05, 4.526825592387652e-05, 3.557930252211e-05, 2.2419423311453615e-05, 3.075296364907836e-05, 3.3001313424926466e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 2.689284403348397e-05, 2.2419423311453615e-05, 2.2323109687723337e-05, 3.557930252211e-05, 2.2323109687723337e-05, 2.2323109687723337e-05, 4.526825592387652e-05, 2.689284403348397e-05, 3.3001313424926466e-05, 1.7230192411927546e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.7230192411927546e-05, 5.237261002037342e-05, 2.689284403348397e-05, 1.7230192411927546e-05, 2.2323109687723337e-05, 1.6273919204274127e-05, 3.075296364907836e-05, 2.689284403348397e-05, 5.237261002037342e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 2.593790482997429e-05, 1.882793155145546e-05, 5.237261002037342e-05, 2.5826475843981168e-05, 3.075296364907836e-05, 2.5826475843981168e-05, 1.7971365461170967e-05, 2.363483991311807e-05, 1.9605331743486973e-05, 2.689284403348397e-05, 3.075296364907836e-05, 1.9605331743486973e-05, 2.2419423311453615e-05, 3.111337965555469e-05, 2.689284403348397e-05, 2.5826475843981168e-05, 3.075296364907836e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 10\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.012778210039538e-05, 1.8450273261465618e-05, 2.3016864653635486e-05, 2.760949479703167e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 2.760949479703167e-05, 3.652743334409438e-05, 2.760949479703167e-05, 1.7689349150155283e-05, 1.670759280938393e-05, 2.291798442773028e-05, 1.93296654515805e-05, 3.157248035222162e-05, 2.291798442773028e-05, 3.157248035222162e-05, 1.7689349150155283e-05, 3.1942500861885285e-05, 4.647458167105024e-05, 3.157248035222162e-05, 2.012778210039538e-05, 3.1942500861885285e-05, 4.4122970198274376e-05, 2.760949479703167e-05, 2.291798442773028e-05, 2.291798442773028e-05, 2.291798442773028e-05, 3.157248035222162e-05, 3.652743334409438e-05, 2.760949479703167e-05, 3.652743334409438e-05, 1.93296654515805e-05, 2.662910800945521e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 4.647458167105024e-05, 2.012778210039538e-05, 2.3016864653635486e-05, 4.647458167105024e-05, 2.3016864653635486e-05, 1.670759280938393e-05, 1.7689349150155283e-05, 2.012778210039538e-05, 3.157248035222162e-05, 3.157248035222162e-05, 3.157248035222162e-05, 5.376825530479759e-05, 3.157248035222162e-05, 3.157248035222162e-05, 4.647458167105024e-05, 3.652743334409438e-05, 2.3016864653635486e-05, 3.157248035222162e-05, 3.388074501031787e-05, 2.426467014040733e-05, 2.012778210039538e-05, 2.621245576219321e-05, 2.760949479703167e-05, 2.3016864653635486e-05, 2.291798442773028e-05, 3.652743334409438e-05, 2.291798442773028e-05, 2.291798442773028e-05, 4.647458167105024e-05, 2.760949479703167e-05, 3.388074501031787e-05, 1.7689349150155283e-05, 2.012778210039538e-05, 2.760949479703167e-05, 3.157248035222162e-05, 1.7689349150155283e-05, 5.104758431649547e-05, 2.760949479703167e-05, 1.7689349150155283e-05, 2.291798442773028e-05, 1.670759280938393e-05, 3.157248035222162e-05, 2.760949479703167e-05, 5.376825530479759e-05, 5.376825530479759e-05, 2.517306666081184e-05, 2.662910800945521e-05, 1.93296654515805e-05, 5.376825530479759e-05, 2.6514709621349248e-05, 3.157248035222162e-05, 2.6514709621349248e-05, 1.8450273261465618e-05, 2.426467014040733e-05, 2.012778210039538e-05, 2.760949479703167e-05, 3.157248035222162e-05, 2.012778210039538e-05, 2.3016864653635486e-05, 3.1942500861885285e-05, 2.760949479703167e-05, 2.6514709621349248e-05, 3.157248035222162e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 11\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9527038425207157e-05, 1.7899597339397576e-05, 2.232989200089282e-05, 2.6785448248250064e-05, 3.063015258918263e-05, 3.063015258918263e-05, 3.063015258918263e-05, 2.6785448248250064e-05, 3.543721761923847e-05, 2.6785448248250064e-05, 1.7161384143025e-05, 1.7237912179357335e-05, 2.3645429200980195e-05, 1.9943212604724033e-05, 3.063015258918263e-05, 2.3645429200980195e-05, 3.063015258918263e-05, 1.7161384143025e-05, 3.0989129284889974e-05, 4.508747846928448e-05, 3.063015258918263e-05, 1.9527038425207157e-05, 3.0989129284889974e-05, 4.2806054348086825e-05, 2.6785448248250064e-05, 2.3645429200980195e-05, 2.3645429200980195e-05, 2.3645429200980195e-05, 3.063015258918263e-05, 3.543721761923847e-05, 2.6785448248250064e-05, 3.543721761923847e-05, 1.9943212604724033e-05, 2.5834322566489284e-05, 3.063015258918263e-05, 3.063015258918263e-05, 3.063015258918263e-05, 4.508747846928448e-05, 1.9527038425207157e-05, 2.232989200089282e-05, 4.508747846928448e-05, 2.232989200089282e-05, 1.7237912179357335e-05, 1.7161384143025e-05, 1.9527038425207157e-05, 3.063015258918263e-05, 3.063015258918263e-05, 3.063015258918263e-05, 5.216346153571021e-05, 3.063015258918263e-05, 3.063015258918263e-05, 4.508747846928448e-05, 3.543721761923847e-05, 2.232989200089282e-05, 3.063015258918263e-05, 3.286952364603185e-05, 2.3540454871945538e-05, 1.9527038425207157e-05, 2.5430105926938413e-05, 2.6785448248250064e-05, 2.232989200089282e-05, 2.3645429200980195e-05, 3.543721761923847e-05, 2.3645429200980195e-05, 2.3645429200980195e-05, 4.508747846928448e-05, 2.6785448248250064e-05, 3.286952364603185e-05, 1.7161384143025e-05, 1.9527038425207157e-05, 2.6785448248250064e-05, 3.063015258918263e-05, 1.7161384143025e-05, 4.9523993030639394e-05, 2.6785448248250064e-05, 1.7161384143025e-05, 2.3645429200980195e-05, 1.7237912179357335e-05, 3.063015258918263e-05, 2.6785448248250064e-05, 5.216346153571021e-05, 5.216346153571021e-05, 2.5972090494117277e-05, 2.5834322566489284e-05, 1.9943212604724033e-05, 5.216346153571021e-05, 2.735631883829905e-05, 3.063015258918263e-05, 2.735631883829905e-05, 1.7899597339397576e-05, 2.3540454871945538e-05, 1.9527038425207157e-05, 2.6785448248250064e-05, 3.063015258918263e-05, 1.9527038425207157e-05, 2.232989200089282e-05, 3.0989129284889974e-05, 2.6785448248250064e-05, 2.735631883829905e-05, 3.063015258918263e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 12\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9061324098719876e-05, 1.7472697021090725e-05, 2.28891288713697e-05, 2.6146622906743554e-05, 3.139726380832865e-05, 3.139726380832865e-05, 3.139726380832865e-05, 2.6146622906743554e-05, 3.632471849380601e-05, 2.6146622906743554e-05, 1.675209000001552e-05, 1.682679286439285e-05, 2.3081492422906265e-05, 1.9467572642126726e-05, 3.139726380832865e-05, 2.3081492422906265e-05, 3.139726380832865e-05, 1.675209000001552e-05, 3.025004734327257e-05, 4.4012154899417035e-05, 3.139726380832865e-05, 1.9061324098719876e-05, 3.025004734327257e-05, 4.1785142096254374e-05, 2.6146622906743554e-05, 2.3081492422906265e-05, 2.3081492422906265e-05, 2.3081492422906265e-05, 3.139726380832865e-05, 3.632471849380601e-05, 2.6146622906743554e-05, 3.632471849380601e-05, 1.9467572642126726e-05, 2.6481325503287904e-05, 3.139726380832865e-05, 3.139726380832865e-05, 3.139726380832865e-05, 4.4012154899417035e-05, 1.9061324098719876e-05, 2.28891288713697e-05, 4.4012154899417035e-05, 2.28891288713697e-05, 1.682679286439285e-05, 1.675209000001552e-05, 1.9061324098719876e-05, 3.139726380832865e-05, 3.139726380832865e-05, 3.139726380832865e-05, 5.091937777721315e-05, 3.139726380832865e-05, 3.139726380832865e-05, 4.4012154899417035e-05, 3.632471849380601e-05, 2.28891288713697e-05, 3.139726380832865e-05, 3.208559483238191e-05, 2.2979021701837154e-05, 1.9061324098719876e-05, 2.4823605115275334e-05, 2.6146622906743554e-05, 2.28891288713697e-05, 2.3081492422906265e-05, 3.632471849380601e-05, 2.3081492422906265e-05, 2.3081492422906265e-05, 4.4012154899417035e-05, 2.6146622906743554e-05, 3.208559483238191e-05, 1.675209000001552e-05, 1.9061324098719876e-05, 2.6146622906743554e-05, 3.139726380832865e-05, 1.675209000001552e-05, 4.834285984715306e-05, 2.6146622906743554e-05, 1.675209000001552e-05, 2.3081492422906265e-05, 1.682679286439285e-05, 3.139726380832865e-05, 2.6146622906743554e-05, 5.091937777721315e-05, 5.091937777721315e-05, 2.5352663504291698e-05, 2.6481325503287904e-05, 1.9467572642126726e-05, 5.091937777721315e-05, 2.6703878395179752e-05, 3.139726380832865e-05, 2.6703878395179752e-05, 1.7472697021090725e-05, 2.2979021701837154e-05, 1.9061324098719876e-05, 2.6146622906743554e-05, 3.139726380832865e-05, 1.9061324098719876e-05, 2.28891288713697e-05, 3.025004734327257e-05, 2.6146622906743554e-05, 2.6703878395179752e-05, 3.139726380832865e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 13\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9601149594862333e-05, 1.796753186517118e-05, 2.3537359565379334e-05, 2.6887107335316405e-05, 3.228644881064361e-05, 3.228644881064361e-05, 3.228644881064361e-05, 2.6887107335316405e-05, 3.7353451287057806e-05, 2.6887107335316405e-05, 1.7226516920666262e-05, 1.7303335404641523e-05, 2.373517094147993e-05, 2.0018903284520614e-05, 3.228644881064361e-05, 2.373517094147993e-05, 3.228644881064361e-05, 1.7226516920666262e-05, 3.110674264580465e-05, 4.5258599439776746e-05, 3.228644881064361e-05, 1.9601149594862333e-05, 3.110674264580465e-05, 4.0665202025748647e-05, 2.6887107335316405e-05, 2.373517094147993e-05, 2.373517094147993e-05, 2.373517094147993e-05, 3.228644881064361e-05, 3.7353451287057806e-05, 2.6887107335316405e-05, 3.7353451287057806e-05, 2.0018903284520614e-05, 2.723128886387534e-05, 3.228644881064361e-05, 3.228644881064361e-05, 3.228644881064361e-05, 4.5258599439776746e-05, 1.9601149594862333e-05, 2.3537359565379334e-05, 4.5258599439776746e-05, 2.3537359565379334e-05, 1.7303335404641523e-05, 1.7226516920666262e-05, 1.9601149594862333e-05, 3.228644881064361e-05, 3.228644881064361e-05, 3.228644881064361e-05, 5.2361438057469085e-05, 3.228644881064361e-05, 3.228644881064361e-05, 4.5258599439776746e-05, 3.7353451287057806e-05, 2.3537359565379334e-05, 3.228644881064361e-05, 3.299427368699409e-05, 2.362979820229525e-05, 1.9601149594862333e-05, 2.4158274122766876e-05, 2.6887107335316405e-05, 2.3537359565379334e-05, 2.373517094147993e-05, 3.7353451287057806e-05, 2.373517094147993e-05, 2.373517094147993e-05, 4.5258599439776746e-05, 2.6887107335316405e-05, 3.299427368699409e-05, 1.7226516920666262e-05, 1.9601149594862333e-05, 2.6887107335316405e-05, 3.228644881064361e-05, 1.7226516920666262e-05, 4.704715751973361e-05, 2.6887107335316405e-05, 1.7226516920666262e-05, 2.373517094147993e-05, 1.7303335404641523e-05, 3.228644881064361e-05, 2.6887107335316405e-05, 5.2361438057469085e-05, 5.2361438057469085e-05, 2.4673152502819007e-05, 2.723128886387534e-05, 2.0018903284520614e-05, 5.2361438057469085e-05, 2.7460144556383836e-05, 3.228644881064361e-05, 2.7460144556383836e-05, 1.796753186517118e-05, 2.362979820229525e-05, 1.9601149594862333e-05, 2.6887107335316405e-05, 3.228644881064361e-05, 1.9601149594862333e-05, 2.3537359565379334e-05, 3.110674264580465e-05, 2.6887107335316405e-05, 2.7460144556383836e-05, 3.228644881064361e-05, ... ]\n",
      "Split on feature emp_length.4 years. (34593, 2631)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34593 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (2631 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 14\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9175991293896658e-05, 1.8374929516427492e-05, 2.302682298926658e-05, 2.6303913129423442e-05, 3.158614030811851e-05, 3.158614030811851e-05, 3.158614030811851e-05, 2.6303913129423442e-05, 3.654323707959253e-05, 2.7496748351104293e-05, 1.6852865537103196e-05, 1.6928017791453933e-05, 2.3220343742096962e-05, 1.9584683706405975e-05, 3.158614030811851e-05, 2.3220343742096962e-05, 3.3018515046093585e-05, 1.6852865537103196e-05, 3.0432022533708094e-05, 4.4276918791466954e-05, 3.158614030811851e-05, 1.9175991293896658e-05, 3.0432022533708094e-05, 3.978315436226747e-05, 2.6303913129423442e-05, 2.3220343742096962e-05, 2.3220343742096962e-05, 2.3220343742096962e-05, 3.158614030811851e-05, 3.654323707959253e-05, 2.6303913129423442e-05, 3.654323707959253e-05, 1.9584683706405975e-05, 2.6640629196163533e-05, 3.158614030811851e-05, 3.158614030811851e-05, 3.158614030811851e-05, 4.4276918791466954e-05, 1.9175991293896658e-05, 2.302682298926658e-05, 4.4276918791466954e-05, 2.302682298926658e-05, 1.6928017791453933e-05, 1.6852865537103196e-05, 1.9175991293896658e-05, 3.158614030811851e-05, 3.158614030811851e-05, 3.158614030811851e-05, 5.1225693445506724e-05, 3.158614030811851e-05, 3.158614030811851e-05, 4.62847975425143e-05, 3.654323707959253e-05, 2.302682298926658e-05, 3.158614030811851e-05, 3.227861212467247e-05, 2.3117256587976722e-05, 1.9175991293896658e-05, 2.363426961320531e-05, 2.6303913129423442e-05, 2.302682298926658e-05, 2.3220343742096962e-05, 3.654323707959253e-05, 2.3220343742096962e-05, 2.3220343742096962e-05, 4.4276918791466954e-05, 2.6303913129423442e-05, 3.227861212467247e-05, 1.6852865537103196e-05, 1.9175991293896658e-05, 2.6303913129423442e-05, 3.158614030811851e-05, 1.6852865537103196e-05, 4.6026682192021334e-05, 2.6303913129423442e-05, 1.6852865537103196e-05, 2.3220343742096962e-05, 1.6928017791453933e-05, 3.158614030811851e-05, 2.6303913129423442e-05, 5.354868664793165e-05, 5.1225693445506724e-05, 2.4137980035163584e-05, 2.6640629196163533e-05, 1.9584683706405975e-05, 5.354868664793165e-05, 2.686452089934465e-05, 3.158614030811851e-05, 2.8082778676605722e-05, 1.8374929516427492e-05, 2.3117256587976722e-05, 1.9175991293896658e-05, 2.6303913129423442e-05, 3.158614030811851e-05, 1.9175991293896658e-05, 2.302682298926658e-05, 3.0432022533708094e-05, 2.6303913129423442e-05, 2.686452089934465e-05, 3.158614030811851e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 15\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9514445739805173e-05, 1.8699245297175155e-05, 2.3433244253044806e-05, 2.6768174726490307e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 2.6768174726490307e-05, 3.718822178301245e-05, 2.798206337783884e-05, 1.7150317031521973e-05, 1.7226795716105972e-05, 2.363018062899303e-05, 1.9930351535023256e-05, 3.2143632718942425e-05, 2.363018062899303e-05, 3.3601288736557476e-05, 1.7150317031521973e-05, 3.0969144874173484e-05, 4.505840225098813e-05, 3.2143632718942425e-05, 1.9514445739805173e-05, 3.0969144874173484e-05, 3.910492673100485e-05, 2.6768174726490307e-05, 2.363018062899303e-05, 2.363018062899303e-05, 2.363018062899303e-05, 3.2143632718942425e-05, 3.718822178301245e-05, 2.6768174726490307e-05, 3.718822178301245e-05, 1.9930351535023256e-05, 2.711083380019418e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 4.505840225098813e-05, 1.9514445739805173e-05, 2.3433244253044806e-05, 4.505840225098813e-05, 2.3433244253044806e-05, 1.7226795716105972e-05, 1.7150317031521973e-05, 1.9514445739805173e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 5.212982212525309e-05, 3.2143632718942425e-05, 3.2143632718942425e-05, 4.7101719873472256e-05, 3.718822178301245e-05, 2.3433244253044806e-05, 3.2143632718942425e-05, 3.2848326598042564e-05, 2.3525273996281395e-05, 1.9514445739805173e-05, 2.3231349961575335e-05, 2.6768174726490307e-05, 2.3433244253044806e-05, 2.363018062899303e-05, 3.718822178301245e-05, 2.363018062899303e-05, 2.363018062899303e-05, 4.505840225098813e-05, 2.6768174726490307e-05, 3.2848326598042564e-05, 1.7150317031521973e-05, 1.9514445739805173e-05, 2.6768174726490307e-05, 3.2143632718942425e-05, 1.7150317031521973e-05, 4.524201420532243e-05, 2.6768174726490307e-05, 1.7150317031521973e-05, 2.363018062899303e-05, 1.7226795716105972e-05, 3.2143632718942425e-05, 2.6768174726490307e-05, 5.4493815939595594e-05, 5.212982212525309e-05, 2.372647307235119e-05, 2.711083380019418e-05, 1.9930351535023256e-05, 5.4493815939595594e-05, 2.7338677170915308e-05, 3.2143632718942425e-05, 2.8578437083563136e-05, 1.8699245297175155e-05, 2.3525273996281395e-05, 1.9514445739805173e-05, 2.6768174726490307e-05, 3.2143632718942425e-05, 1.9514445739805173e-05, 2.3433244253044806e-05, 3.0969144874173484e-05, 2.6768174726490307e-05, 2.7338677170915308e-05, 3.2143632718942425e-05, ... ]\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 16\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.98991302647242e-05, 1.8344612287159934e-05, 2.2988830491322855e-05, 2.729585061925754e-05, 3.153402559080532e-05, 3.153402559080532e-05, 3.153402559080532e-05, 2.729585061925754e-05, 3.648294353148774e-05, 2.853366842469914e-05, 1.6825059597066972e-05, 1.690008785594207e-05, 2.3182031949702262e-05, 1.955237047518901e-05, 3.153402559080532e-05, 2.3182031949702262e-05, 3.296403701994215e-05, 1.6825059597066972e-05, 3.157963368548363e-05, 4.4203865259639156e-05, 3.153402559080532e-05, 1.98991302647242e-05, 3.157963368548363e-05, 3.8363297983285506e-05, 2.729585061925754e-05, 2.3182031949702262e-05, 2.3182031949702262e-05, 2.3182031949702262e-05, 3.153402559080532e-05, 3.648294353148774e-05, 2.729585061925754e-05, 3.648294353148774e-05, 1.955237047518901e-05, 2.659667419418924e-05, 3.153402559080532e-05, 3.153402559080532e-05, 3.153402559080532e-05, 4.4203865259639156e-05, 1.98991302647242e-05, 2.2988830491322855e-05, 4.4203865259639156e-05, 2.2988830491322855e-05, 1.690008785594207e-05, 1.6825059597066972e-05, 1.98991302647242e-05, 3.153402559080532e-05, 3.153402559080532e-05, 3.153402559080532e-05, 5.114117496660924e-05, 3.153402559080532e-05, 3.153402559080532e-05, 4.620843116421367e-05, 3.648294353148774e-05, 2.2988830491322855e-05, 3.153402559080532e-05, 3.222535488178904e-05, 2.3079114881507153e-05, 1.98991302647242e-05, 2.368930510631036e-05, 2.729585061925754e-05, 2.2988830491322855e-05, 2.3182031949702262e-05, 3.648294353148774e-05, 2.3182031949702262e-05, 2.3182031949702262e-05, 4.4203865259639156e-05, 2.729585061925754e-05, 3.222535488178904e-05, 1.6825059597066972e-05, 1.98991302647242e-05, 2.729585061925754e-05, 3.153402559080532e-05, 1.6825059597066972e-05, 4.438399499535952e-05, 2.729585061925754e-05, 1.6825059597066972e-05, 2.3182031949702262e-05, 1.690008785594207e-05, 3.153402559080532e-05, 2.729585061925754e-05, 5.346033540780142e-05, 5.114117496660924e-05, 2.3276498197484764e-05, 2.659667419418924e-05, 1.955237047518901e-05, 5.346033540780142e-05, 2.6820196493172625e-05, 3.153402559080532e-05, 2.8036444238215218e-05, 1.8344612287159934e-05, 2.3079114881507153e-05, 1.9144352373923113e-05, 2.729585061925754e-05, 3.153402559080532e-05, 1.98991302647242e-05, 2.2988830491322855e-05, 3.157963368548363e-05, 2.729585061925754e-05, 2.6820196493172625e-05, 3.153402559080532e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 17\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9611795656824086e-05, 1.8079724228813243e-05, 2.2656881983652844e-05, 2.6901710652804438e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 2.6901710652804438e-05, 3.5956146021485755e-05, 2.8121654918596474e-05, 1.658211320504353e-05, 1.6656058089165132e-05, 2.2847293698734397e-05, 1.9270042924724888e-05, 3.1078688259067336e-05, 2.2847293698734397e-05, 3.2488050957942216e-05, 1.658211320504353e-05, 3.112363779310369e-05, 4.48611306943613e-05, 3.1078688259067336e-05, 1.9611795656824086e-05, 3.112363779310369e-05, 3.8933720266003444e-05, 2.6901710652804438e-05, 2.2847293698734397e-05, 2.2847293698734397e-05, 2.2847293698734397e-05, 3.1078688259067336e-05, 3.5956146021485755e-05, 2.6901710652804438e-05, 3.5956146021485755e-05, 1.9270042924724888e-05, 2.621263002495327e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 4.48611306943613e-05, 1.9611795656824086e-05, 2.2656881983652844e-05, 4.48611306943613e-05, 2.2656881983652844e-05, 1.6656058089165132e-05, 1.658211320504353e-05, 1.9611795656824086e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 5.190159097093818e-05, 3.1078688259067336e-05, 3.1078688259067336e-05, 4.689550240602894e-05, 3.5956146021485755e-05, 2.2656881983652844e-05, 3.1078688259067336e-05, 3.270451234372244e-05, 2.274586270731966e-05, 1.9611795656824086e-05, 2.334724205613704e-05, 2.6901710652804438e-05, 2.2656881983652844e-05, 2.2847293698734397e-05, 3.5956146021485755e-05, 2.2847293698734397e-05, 2.2847293698734397e-05, 4.48611306943613e-05, 2.6901710652804438e-05, 3.270451234372244e-05, 1.658211320504353e-05, 1.9611795656824086e-05, 2.6901710652804438e-05, 3.1078688259067336e-05, 1.658211320504353e-05, 4.5043938771633e-05, 2.6901710652804438e-05, 1.658211320504353e-05, 2.2847293698734397e-05, 1.6656058089165132e-05, 3.1078688259067336e-05, 2.6901710652804438e-05, 5.425523491230885e-05, 5.190159097093818e-05, 2.2940395895832004e-05, 2.621263002495327e-05, 1.9270042924724888e-05, 5.425523491230885e-05, 2.6432924761159742e-05, 3.1078688259067336e-05, 2.7631610428650095e-05, 1.8079724228813243e-05, 2.274586270731966e-05, 1.886791642372411e-05, 2.6901710652804438e-05, 3.1078688259067336e-05, 1.9611795656824086e-05, 2.2656881983652844e-05, 3.112363779310369e-05, 2.6901710652804438e-05, 2.6432924761159742e-05, 3.1078688259067336e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 18\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0004314935168852e-05, 1.773179571047259e-05, 2.3110346986112803e-05, 2.7440133560958915e-05, 3.170071107129801e-05, 3.170071107129801e-05, 3.170071107129801e-05, 2.7440133560958915e-05, 3.667578846195254e-05, 2.868449433869145e-05, 1.691399505934024e-05, 1.698941990955269e-05, 2.3304569686700224e-05, 1.9655722210540504e-05, 3.170071107129801e-05, 2.3304569686700224e-05, 3.313828138762764e-05, 1.691399505934024e-05, 3.174656024547801e-05, 4.5759001493854045e-05, 3.170071107129801e-05, 2.0004314935168852e-05, 3.174656024547801e-05, 3.9712957213476494e-05, 2.7440133560958915e-05, 2.3304569686700224e-05, 2.3304569686700224e-05, 2.3304569686700224e-05, 3.170071107129801e-05, 3.667578846195254e-05, 2.7440133560958915e-05, 3.667578846195254e-05, 1.9655722210540504e-05, 2.673726136422245e-05, 3.170071107129801e-05, 3.170071107129801e-05, 3.170071107129801e-05, 4.5759001493854045e-05, 2.0004314935168852e-05, 2.3110346986112803e-05, 4.5759001493854045e-05, 2.3110346986112803e-05, 1.698941990955269e-05, 1.691399505934024e-05, 2.0004314935168852e-05, 3.170071107129801e-05, 3.170071107129801e-05, 3.170071107129801e-05, 5.294037270155289e-05, 3.170071107129801e-05, 3.170071107129801e-05, 4.7834090033808184e-05, 3.667578846195254e-05, 2.3110346986112803e-05, 3.170071107129801e-05, 3.335907512871192e-05, 2.32011086099099e-05, 2.0004314935168852e-05, 2.381452423486129e-05, 2.7440133560958915e-05, 2.3110346986112803e-05, 2.3304569686700224e-05, 3.667578846195254e-05, 2.3304569686700224e-05, 2.3304569686700224e-05, 4.5759001493854045e-05, 2.7440133560958915e-05, 3.335907512871192e-05, 1.691399505934024e-05, 2.0004314935168852e-05, 2.7440133560958915e-05, 3.170071107129801e-05, 1.691399505934024e-05, 4.594546837401264e-05, 2.7440133560958915e-05, 1.691399505934024e-05, 2.3304569686700224e-05, 1.698941990955269e-05, 3.170071107129801e-05, 2.7440133560958915e-05, 5.5341123528876965e-05, 5.294037270155289e-05, 2.3399535272946715e-05, 2.673726136422245e-05, 1.9655722210540504e-05, 5.5341123528876965e-05, 2.6961965178128482e-05, 3.170071107129801e-05, 2.818464187843392e-05, 1.773179571047259e-05, 2.32011086099099e-05, 1.9245547369309275e-05, 2.7440133560958915e-05, 3.170071107129801e-05, 2.0004314935168852e-05, 2.3110346986112803e-05, 3.174656024547801e-05, 2.7440133560958915e-05, 2.6961965178128482e-05, 3.170071107129801e-05, ... ]\n",
      "Split on feature term. 36 months. (9223, 28001)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9223 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (28001 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 19\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0431354479926306e-05, 1.8110322942845843e-05, 2.3603692151299445e-05, 2.6878344281779483e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 2.6878344281779483e-05, 3.5924915120989915e-05, 2.8097228924608794e-05, 1.7275064397309527e-05, 1.7352099369828212e-05, 2.2827448925724574e-05, 2.0075320216864204e-05, 3.1051693835916975e-05, 2.2827448925724574e-05, 3.24598323861808e-05, 1.7275064397309527e-05, 3.109660432754806e-05, 4.4822165074742076e-05, 3.1051693835916975e-05, 2.0431354479926306e-05, 3.109660432754806e-05, 3.889990309486298e-05, 2.6878344281779483e-05, 2.2827448925724574e-05, 2.2827448925724574e-05, 2.2827448925724574e-05, 3.1051693835916975e-05, 3.5924915120989915e-05, 2.6878344281779483e-05, 3.5924915120989915e-05, 2.0075320216864204e-05, 2.730803161844222e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 4.4822165074742076e-05, 2.0431354479926306e-05, 2.3603692151299445e-05, 4.4822165074742076e-05, 2.3603692151299445e-05, 1.7352099369828212e-05, 1.7275064397309527e-05, 2.0431354479926306e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 5.185651012656123e-05, 3.1051693835916975e-05, 3.1051693835916975e-05, 4.6854769765537664e-05, 3.5924915120989915e-05, 2.3603692151299445e-05, 3.1051693835916975e-05, 3.407120370210536e-05, 2.2726106035554007e-05, 2.0431354479926306e-05, 2.3326963036436993e-05, 2.6878344281779483e-05, 2.3603692151299445e-05, 2.2827448925724574e-05, 3.5924915120989915e-05, 2.2827448925724574e-05, 2.2827448925724574e-05, 4.4822165074742076e-05, 2.6878344281779483e-05, 3.407120370210536e-05, 1.7275064397309527e-05, 2.0431354479926306e-05, 2.6878344281779483e-05, 3.1051693835916975e-05, 1.7275064397309527e-05, 4.500481436800874e-05, 2.6878344281779483e-05, 1.7275064397309527e-05, 2.2827448925724574e-05, 1.7352099369828212e-05, 3.1051693835916975e-05, 2.6878344281779483e-05, 5.420810973259932e-05, 5.185651012656123e-05, 2.2920470255827934e-05, 2.730803161844222e-05, 2.0075320216864204e-05, 5.420810973259932e-05, 2.6409965569634143e-05, 3.1051693835916975e-05, 2.7607610079020822e-05, 1.8110322942845843e-05, 2.2726106035554007e-05, 1.9656389220871458e-05, 2.6878344281779483e-05, 3.1051693835916975e-05, 2.0431354479926306e-05, 2.3603692151299445e-05, 3.109660432754806e-05, 2.6878344281779483e-05, 2.6409965569634143e-05, 3.1051693835916975e-05, ... ]\n",
      "Split on feature grade.B. (26858, 10366)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (26858 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (10366 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 20\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.014165627880483e-05, 1.785353487803956e-05, 2.3948139138803386e-05, 2.649723357298112e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 2.649723357298112e-05, 3.644916483202839e-05, 2.76988355295965e-05, 1.703011954624372e-05, 1.710606223224836e-05, 2.2503776263862036e-05, 1.979067026086265e-05, 3.1504828978085296e-05, 2.2503776263862036e-05, 3.293351639326896e-05, 1.703011954624372e-05, 3.0655682491282724e-05, 4.41866271516308e-05, 3.1504828978085296e-05, 2.014165627880483e-05, 3.0655682491282724e-05, 3.834833751160938e-05, 2.649723357298112e-05, 2.2503776263862036e-05, 2.2503776263862036e-05, 2.2503776263862036e-05, 3.1504828978085296e-05, 3.644916483202839e-05, 2.649723357298112e-05, 3.644916483202839e-05, 1.979067026086265e-05, 2.7706535766240003e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 4.41866271516308e-05, 2.014165627880483e-05, 2.3948139138803386e-05, 4.41866271516308e-05, 2.3948139138803386e-05, 1.710606223224836e-05, 1.703011954624372e-05, 2.014165627880483e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 5.11212315274423e-05, 3.1504828978085296e-05, 3.1504828978085296e-05, 4.619041133896475e-05, 3.644916483202839e-05, 2.3948139138803386e-05, 3.1504828978085296e-05, 3.358810472635018e-05, 2.240387032458025e-05, 2.014165627880483e-05, 2.2996207714467396e-05, 2.649723357298112e-05, 2.3948139138803386e-05, 2.2503776263862036e-05, 3.644916483202839e-05, 2.2503776263862036e-05, 2.2503776263862036e-05, 4.41866271516308e-05, 2.649723357298112e-05, 3.358810472635018e-05, 1.703011954624372e-05, 2.014165627880483e-05, 2.649723357298112e-05, 3.1504828978085296e-05, 1.703011954624372e-05, 4.436668664245693e-05, 2.649723357298112e-05, 1.703011954624372e-05, 2.2503776263862036e-05, 1.710606223224836e-05, 3.1504828978085296e-05, 2.649723357298112e-05, 5.3439487569484354e-05, 5.11212315274423e-05, 2.259547863530197e-05, 2.7706535766240003e-05, 1.979067026086265e-05, 5.3439487569484354e-05, 2.603549604903921e-05, 3.1504828978085296e-05, 2.7216159038170184e-05, 1.785353487803956e-05, 2.240387032458025e-05, 1.937767932900282e-05, 2.649723357298112e-05, 3.1504828978085296e-05, 2.014165627880483e-05, 2.3948139138803386e-05, 3.0655682491282724e-05, 2.649723357298112e-05, 2.603549604903921e-05, 3.1504828978085296e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 21\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0500122930295267e-05, 1.817127919739493e-05, 2.4374350823070265e-05, 2.6968812198948528e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 2.6968812198948528e-05, 3.709786074293565e-05, 2.8191799399351705e-05, 1.733320931422114e-05, 1.7410503573303005e-05, 2.290428222084659e-05, 2.014289031669928e-05, 3.2065529170424356e-05, 2.290428222084659e-05, 3.351964333237891e-05, 1.733320931422114e-05, 3.120127018773091e-05, 4.497302882865523e-05, 3.2065529170424356e-05, 2.0500122930295267e-05, 3.120127018773091e-05, 3.7689299656515175e-05, 2.6968812198948528e-05, 2.290428222084659e-05, 2.290428222084659e-05, 2.290428222084659e-05, 3.2065529170424356e-05, 3.709786074293565e-05, 2.6968812198948528e-05, 3.709786074293565e-05, 2.014289031669928e-05, 2.8199636679245626e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 4.497302882865523e-05, 2.0500122930295267e-05, 2.4374350823070265e-05, 4.497302882865523e-05, 2.4374350823070265e-05, 1.7410503573303005e-05, 1.733320931422114e-05, 2.0500122930295267e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 5.203105028475043e-05, 3.2065529170424356e-05, 3.2065529170424356e-05, 4.701247491975718e-05, 3.709786074293565e-05, 2.4374350823070265e-05, 3.2065529170424356e-05, 3.4185881555847315e-05, 2.2802598227812786e-05, 2.0500122930295267e-05, 2.2601004887151735e-05, 2.6968812198948528e-05, 2.4374350823070265e-05, 2.290428222084659e-05, 3.709786074293565e-05, 2.290428222084659e-05, 2.290428222084659e-05, 4.497302882865523e-05, 2.6968812198948528e-05, 3.4185881555847315e-05, 1.733320931422114e-05, 2.0500122930295267e-05, 2.6968812198948528e-05, 3.2065529170424356e-05, 1.733320931422114e-05, 4.3604220055903354e-05, 2.6968812198948528e-05, 1.733320931422114e-05, 2.290428222084659e-05, 1.7410503573303005e-05, 3.2065529170424356e-05, 2.6968812198948528e-05, 5.439056497350878e-05, 5.203105028475043e-05, 2.220716256370882e-05, 2.8199636679245626e-05, 2.014289031669928e-05, 5.439056497350878e-05, 2.649885700403737e-05, 3.2065529170424356e-05, 2.7700532580335656e-05, 1.817127919739493e-05, 2.2802598227812786e-05, 1.9722549270509703e-05, 2.6968812198948528e-05, 3.2065529170424356e-05, 2.0500122930295267e-05, 2.4374350823070265e-05, 3.120127018773091e-05, 2.6968812198948528e-05, 2.649885700403737e-05, 3.2065529170424356e-05, ... ]\n",
      "Split on feature grade.D. (30465, 6759)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (30465 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (6759 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 22\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0151106561790354e-05, 1.7861911595155275e-05, 2.3959375389124898e-05, 2.650966583535975e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 2.650966583535975e-05, 3.64662664505587e-05, 2.771183157274712e-05, 1.703810992433188e-05, 1.7717367680664527e-05, 2.3307975433329763e-05, 2.0497913365325688e-05, 3.1519610758944444e-05, 2.3307975433329763e-05, 3.294896850134372e-05, 1.703810992433188e-05, 3.0670065860289074e-05, 4.42073591175118e-05, 3.1519610758944444e-05, 2.0151106561790354e-05, 3.0670065860289074e-05, 3.70476360654071e-05, 2.650966583535975e-05, 2.3307975433329763e-05, 2.3307975433329763e-05, 2.3307975433329763e-05, 3.1519610758944444e-05, 3.64662664505587e-05, 2.650966583535975e-05, 3.64662664505587e-05, 2.0497913365325688e-05, 2.771953542227202e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 4.42073591175118e-05, 2.0151106561790354e-05, 2.3959375389124898e-05, 4.42073591175118e-05, 2.3959375389124898e-05, 1.7717367680664527e-05, 1.703810992433188e-05, 2.0151106561790354e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 5.114521714698697e-05, 3.1519610758944444e-05, 3.1519610758944444e-05, 4.621208346226627e-05, 3.64662664505587e-05, 2.3959375389124898e-05, 3.1519610758944444e-05, 3.3603863961351405e-05, 2.2414382017938912e-05, 2.0151106561790354e-05, 2.2216220821363608e-05, 2.650966583535975e-05, 2.3959375389124898e-05, 2.3307975433329763e-05, 3.64662664505587e-05, 2.3307975433329763e-05, 2.3307975433329763e-05, 4.42073591175118e-05, 2.650966583535975e-05, 3.3603863961351405e-05, 1.703810992433188e-05, 2.0151106561790354e-05, 2.650966583535975e-05, 3.1519610758944444e-05, 1.703810992433188e-05, 4.2861854432675296e-05, 2.650966583535975e-05, 1.703810992433188e-05, 2.3307975433329763e-05, 1.7717367680664527e-05, 3.1519610758944444e-05, 2.650966583535975e-05, 5.346456089380002e-05, 5.114521714698697e-05, 2.259856888279968e-05, 2.771953542227202e-05, 2.0497913365325688e-05, 5.346456089380002e-05, 2.696590541917416e-05, 3.1519610758944444e-05, 2.81887608023348e-05, 1.7861911595155275e-05, 2.2414382017938912e-05, 1.9386771160912126e-05, 2.650966583535975e-05, 3.1519610758944444e-05, 2.0151106561790354e-05, 2.3959375389124898e-05, 3.0670065860289074e-05, 2.650966583535975e-05, 2.696590541917416e-05, 3.1519610758944444e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 23\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0382259818772148e-05, 1.7661612933992744e-05, 2.4234213281498865e-05, 2.6813758098509294e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 2.6813758098509294e-05, 3.688457083668089e-05, 2.8029713873916505e-05, 1.7233554010230912e-05, 1.792060352937445e-05, 2.3575341119603376e-05, 2.073304484166406e-05, 3.1881172078834616e-05, 2.3575341119603376e-05, 3.332692597776925e-05, 1.7233554010230912e-05, 3.10218820542888e-05, 4.471446154443028e-05, 3.1881172078834616e-05, 2.0382259818772148e-05, 3.10218820542888e-05, 3.7472609339888866e-05, 2.6813758098509294e-05, 2.3575341119603376e-05, 2.3575341119603376e-05, 2.3575341119603376e-05, 3.1881172078834616e-05, 3.688457083668089e-05, 2.6813758098509294e-05, 3.688457083668089e-05, 2.073304484166406e-05, 2.803750609426628e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 4.471446154443028e-05, 2.0382259818772148e-05, 2.4234213281498865e-05, 4.471446154443028e-05, 2.4234213281498865e-05, 1.792060352937445e-05, 1.7233554010230912e-05, 2.0382259818772148e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 5.173190371361871e-05, 3.1881172078834616e-05, 3.1881172078834616e-05, 4.67421820735491e-05, 3.688457083668089e-05, 2.4234213281498865e-05, 3.1881172078834616e-05, 3.398933374079112e-05, 2.2671497297975444e-05, 2.0382259818772148e-05, 2.2471062995163748e-05, 2.6813758098509294e-05, 2.4234213281498865e-05, 2.3575341119603376e-05, 3.688457083668089e-05, 2.3575341119603376e-05, 2.3575341119603376e-05, 4.471446154443028e-05, 2.6813758098509294e-05, 3.398933374079112e-05, 1.7233554010230912e-05, 2.0382259818772148e-05, 2.6813758098509294e-05, 3.1881172078834616e-05, 1.7233554010230912e-05, 4.335352258112224e-05, 2.6813758098509294e-05, 1.7233554010230912e-05, 2.3575341119603376e-05, 1.792060352937445e-05, 3.1881172078834616e-05, 2.6813758098509294e-05, 5.407785264260835e-05, 5.173190371361871e-05, 2.2857796969573413e-05, 2.803750609426628e-05, 2.073304484166406e-05, 5.407785264260835e-05, 2.7275231204633718e-05, 3.1881172078834616e-05, 2.8512113956652173e-05, 1.7661612933992744e-05, 2.2671497297975444e-05, 1.960915672978718e-05, 2.6813758098509294e-05, 3.1881172078834616e-05, 2.0382259818772148e-05, 2.4234213281498865e-05, 3.10218820542888e-05, 2.6813758098509294e-05, 2.7275231204633718e-05, 3.1881172078834616e-05, ... ]\n",
      "Split on feature grade.A. (32094, 5130)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (32094 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (5130 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 24\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0023555222316973e-05, 1.735078862910382e-05, 2.3807718684096942e-05, 2.634186644548886e-05, 3.132009970183922e-05, 3.132009970183922e-05, 3.132009970183922e-05, 2.634186644548886e-05, 3.623544433083543e-05, 2.7536422781893576e-05, 1.6930263055660956e-05, 1.7605221284501043e-05, 2.3160441923057378e-05, 2.036816682767826e-05, 3.132009970183922e-05, 2.3160441923057378e-05, 3.27404099760972e-05, 1.6930263055660956e-05, 3.0475932204639896e-05, 4.553009361757194e-05, 3.132009970183922e-05, 2.0023555222316973e-05, 3.0475932204639896e-05, 3.815614350280283e-05, 2.634186644548886e-05, 2.3160441923057378e-05, 2.3160441923057378e-05, 2.3160441923057378e-05, 3.132009970183922e-05, 3.623544433083543e-05, 2.634186644548886e-05, 3.623544433083543e-05, 2.036816682767826e-05, 2.754407786802561e-05, 3.132009970183922e-05, 3.132009970183922e-05, 3.132009970183922e-05, 4.553009361757194e-05, 2.0023555222316973e-05, 2.3807718684096942e-05, 4.553009361757194e-05, 2.3807718684096942e-05, 1.7605221284501043e-05, 1.6930263055660956e-05, 2.0023555222316973e-05, 3.132009970183922e-05, 3.132009970183922e-05, 3.132009970183922e-05, 5.267554025571545e-05, 3.132009970183922e-05, 3.132009970183922e-05, 4.759480159642833e-05, 3.623544433083543e-05, 2.3807718684096942e-05, 3.132009970183922e-05, 3.460932981781329e-05, 2.227250472494314e-05, 2.0023555222316973e-05, 2.2075597837950156e-05, 2.634186644548886e-05, 2.3807718684096942e-05, 2.3160441923057378e-05, 3.623544433083543e-05, 2.3160441923057378e-05, 2.3160441923057378e-05, 4.553009361757194e-05, 2.634186644548886e-05, 3.460932981781329e-05, 1.6930263055660956e-05, 2.0023555222316973e-05, 2.634186644548886e-05, 3.132009970183922e-05, 1.6930263055660956e-05, 4.4144329901158925e-05, 2.634186644548886e-05, 1.6930263055660956e-05, 2.3160441923057378e-05, 1.7605221284501043e-05, 3.132009970183922e-05, 2.634186644548886e-05, 5.506428140722878e-05, 5.267554025571545e-05, 2.2455525734159485e-05, 2.754407786802561e-05, 2.036816682767826e-05, 5.506428140722878e-05, 2.679521814967949e-05, 3.132009970183922e-05, 2.8010333171702837e-05, 1.735078862910382e-05, 2.227250472494314e-05, 1.926405786861447e-05, 2.634186644548886e-05, 3.132009970183922e-05, 2.0023555222316973e-05, 2.3807718684096942e-05, 3.0475932204639896e-05, 2.634186644548886e-05, 2.679521814967949e-05, 3.132009970183922e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 25\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0289042804272142e-05, 1.7580838631064608e-05, 2.4123379594216702e-05, 2.669112702130331e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 2.669112702130331e-05, 3.6715881515424055e-05, 2.7901521697590238e-05, 1.715473740852227e-05, 1.7838644748851596e-05, 2.3467520743721182e-05, 2.0638223533388188e-05, 3.1735365494757385e-05, 2.3467520743721182e-05, 3.317450732695562e-05, 1.715473740852227e-05, 3.08800053804075e-05, 4.613376635832726e-05, 3.1735365494757385e-05, 2.0289042804272142e-05, 3.08800053804075e-05, 3.7663308975101216e-05, 2.669112702130331e-05, 2.3467520743721182e-05, 2.3467520743721182e-05, 2.3467520743721182e-05, 3.1735365494757385e-05, 3.6715881515424055e-05, 2.669112702130331e-05, 3.6715881515424055e-05, 2.0638223533388188e-05, 2.790927828069842e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 4.613376635832726e-05, 2.0289042804272142e-05, 2.4123379594216702e-05, 4.613376635832726e-05, 2.4123379594216702e-05, 1.7838644748851596e-05, 1.715473740852227e-05, 2.0289042804272142e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 5.337395278313144e-05, 3.1735365494757385e-05, 3.1735365494757385e-05, 4.822584981185157e-05, 3.6715881515424055e-05, 2.4123379594216702e-05, 3.1735365494757385e-05, 3.506820673474458e-05, 2.2567810596346033e-05, 2.0289042804272142e-05, 2.179046375899383e-05, 2.669112702130331e-05, 2.4123379594216702e-05, 2.3467520743721182e-05, 3.6715881515424055e-05, 2.3467520743721182e-05, 2.3467520743721182e-05, 4.613376635832726e-05, 2.669112702130331e-05, 3.506820673474458e-05, 1.715473740852227e-05, 2.0289042804272142e-05, 2.669112702130331e-05, 3.1735365494757385e-05, 1.715473740852227e-05, 4.3574150423352315e-05, 2.669112702130331e-05, 1.715473740852227e-05, 2.3467520743721182e-05, 1.7838644748851596e-05, 3.1735365494757385e-05, 2.669112702130331e-05, 5.5794365688496216e-05, 5.337395278313144e-05, 2.216548440913215e-05, 2.790927828069842e-05, 2.0638223533388188e-05, 5.5794365688496216e-05, 2.715048960849571e-05, 3.1735365494757385e-05, 2.838171555315056e-05, 1.7580838631064608e-05, 2.2567810596346033e-05, 1.9519475454822276e-05, 2.669112702130331e-05, 3.1735365494757385e-05, 2.0289042804272142e-05, 2.4123379594216702e-05, 3.08800053804075e-05, 2.669112702130331e-05, 2.715048960849571e-05, 3.1735365494757385e-05, ... ]\n",
      "Split on feature emp_length.2 years. (33652, 3572)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (33652 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3572 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 26\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9997807148268495e-05, 1.7328477436837434e-05, 2.3777104595000504e-05, 2.6307993723073528e-05, 3.127982552288473e-05, 3.127982552288473e-05, 3.127982552288473e-05, 2.6307993723073528e-05, 3.618884956315053e-05, 2.750101399234814e-05, 1.690849261270128e-05, 1.758258292002279e-05, 2.313066016017814e-05, 2.0341975621277194e-05, 3.22043699581586e-05, 2.313066016017814e-05, 3.269830943545482e-05, 1.690849261270128e-05, 3.0436743531580138e-05, 4.5471546960453186e-05, 3.127982552288473e-05, 1.9997807148268495e-05, 3.0436743531580138e-05, 3.712267733454289e-05, 2.6307993723073528e-05, 2.313066016017814e-05, 2.313066016017814e-05, 2.313066016017814e-05, 3.127982552288473e-05, 3.618884956315053e-05, 2.6307993723073528e-05, 3.618884956315053e-05, 2.0341975621277194e-05, 2.7508659234887343e-05, 3.22043699581586e-05, 3.22043699581586e-05, 3.127982552288473e-05, 4.5471546960453186e-05, 1.9997807148268495e-05, 2.3777104595000504e-05, 4.5471546960453186e-05, 2.3777104595000504e-05, 1.758258292002279e-05, 1.690849261270128e-05, 1.9997807148268495e-05, 3.22043699581586e-05, 3.127982552288473e-05, 3.127982552288473e-05, 5.2607805345706253e-05, 3.127982552288473e-05, 3.22043699581586e-05, 4.7533599953552164e-05, 3.618884956315053e-05, 2.3777104595000504e-05, 3.127982552288473e-05, 3.456482605327082e-05, 2.2243864750945727e-05, 1.9997807148268495e-05, 2.147767620816181e-05, 2.6307993723073528e-05, 2.3777104595000504e-05, 2.313066016017814e-05, 3.618884956315053e-05, 2.313066016017814e-05, 2.313066016017814e-05, 4.5471546960453186e-05, 2.6307993723073528e-05, 3.456482605327082e-05, 1.690849261270128e-05, 1.9997807148268495e-05, 2.6307993723073528e-05, 3.127982552288473e-05, 1.690849261270128e-05, 4.2948672602354546e-05, 2.6307993723073528e-05, 1.690849261270128e-05, 2.313066016017814e-05, 1.758258292002279e-05, 3.127982552288473e-05, 2.7085584671657706e-05, 5.49934748406946e-05, 5.2607805345706253e-05, 2.1847313687388046e-05, 2.7508659234887343e-05, 2.0341975621277194e-05, 5.49934748406946e-05, 2.6760762467189393e-05, 3.127982552288473e-05, 2.797431498589017e-05, 1.7328477436837434e-05, 2.2901331377779227e-05, 1.9239286424034918e-05, 2.6307993723073528e-05, 3.127982552288473e-05, 1.9997807148268495e-05, 2.3777104595000504e-05, 3.0436743531580138e-05, 2.6307993723073528e-05, 2.755173599754443e-05, 3.127982552288473e-05, ... ]\n",
      "Split on feature grade.F. (35512, 1712)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35512 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1712 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 27\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.0234593284607894e-05, 1.7128044279078494e-05, 2.405863989977893e-05, 2.661949628644669e-05, 3.165019758298211e-05, 3.165019758298211e-05, 3.165019758298211e-05, 2.661949628644669e-05, 3.6617347438097185e-05, 2.7826642637548256e-05, 1.7108699395745064e-05, 1.7790771340164633e-05, 2.340454117171607e-05, 2.058283692057668e-05, 3.2585689183766656e-05, 2.340454117171607e-05, 3.30854772033329e-05, 1.7108699395745064e-05, 3.079713299079243e-05, 4.6009957589094214e-05, 3.165019758298211e-05, 2.0234593284607894e-05, 3.079713299079243e-05, 3.756223229531676e-05, 2.661949628644669e-05, 2.340454117171607e-05, 2.340454117171607e-05, 2.340454117171607e-05, 3.165019758298211e-05, 3.6617347438097185e-05, 2.661949628644669e-05, 3.6617347438097185e-05, 2.058283692057668e-05, 2.7834378404384888e-05, 3.2585689183766656e-05, 3.2585689183766656e-05, 3.165019758298211e-05, 4.6009957589094214e-05, 2.0234593284607894e-05, 2.405863989977893e-05, 4.6009957589094214e-05, 2.405863989977893e-05, 1.7790771340164633e-05, 1.7108699395745064e-05, 2.0234593284607894e-05, 3.2585689183766656e-05, 3.165019758298211e-05, 3.165019758298211e-05, 5.323071359143276e-05, 3.165019758298211e-05, 3.2585689183766656e-05, 4.809642653727981e-05, 3.6617347438097185e-05, 2.405863989977893e-05, 3.165019758298211e-05, 3.497409450724259e-05, 2.250724556828142e-05, 2.0234593284607894e-05, 2.173198488956739e-05, 2.661949628644669e-05, 2.405863989977893e-05, 2.340454117171607e-05, 3.6617347438097185e-05, 2.340454117171607e-05, 2.340454117171607e-05, 4.6009957589094214e-05, 2.661949628644669e-05, 3.497409450724259e-05, 1.7108699395745064e-05, 2.0234593284607894e-05, 2.661949628644669e-05, 3.165019758298211e-05, 1.7108699395745064e-05, 4.3457210872126687e-05, 2.661949628644669e-05, 1.7108699395745064e-05, 2.340454117171607e-05, 1.7790771340164633e-05, 3.165019758298211e-05, 2.7406294382344698e-05, 5.564463085669476e-05, 5.323071359143276e-05, 2.210599910019739e-05, 2.7834378404384888e-05, 2.058283692057668e-05, 5.564463085669476e-05, 2.7077626086441306e-05, 3.165019758298211e-05, 2.8305547801225315e-05, 1.7128044279078494e-05, 2.3172496997777823e-05, 1.9467091216055263e-05, 2.661949628644669e-05, 3.165019758298211e-05, 2.0234593284607894e-05, 2.405863989977893e-05, 3.079713299079243e-05, 2.661949628644669e-05, 2.7877965221975494e-05, 3.165019758298211e-05, ... ]\n",
      "Split on feature home_ownership.OWN. (34149, 3075)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (34149 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (3075 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 28\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[1.9962491863571e-05, 1.7364736649484144e-05, 2.373511523027522e-05, 2.6261534914802107e-05, 3.1224586669172496e-05, 3.1224586669172496e-05, 3.1224586669172496e-05, 2.6261534914802107e-05, 3.612494157985406e-05, 2.7452448360556172e-05, 1.6878632927286987e-05, 1.7551532819532563e-05, 2.3089812389082975e-05, 2.0306052549559947e-05, 3.214749839793484e-05, 2.3089812389082975e-05, 3.264056559892923e-05, 1.6878632927286987e-05, 3.0382993525136288e-05, 4.539124612473466e-05, 3.1224586669172496e-05, 1.9962491863571e-05, 3.0382993525136288e-05, 3.808130462224704e-05, 2.6261534914802107e-05, 2.3089812389082975e-05, 2.3089812389082975e-05, 2.3089812389082975e-05, 3.1224586669172496e-05, 3.612494157985406e-05, 2.6261534914802107e-05, 3.612494157985406e-05, 2.0306052549559947e-05, 2.746008010191923e-05, 3.214749839793484e-05, 3.214749839793484e-05, 3.1224586669172496e-05, 4.539124612473466e-05, 1.9962491863571e-05, 2.373511523027522e-05, 4.66457689956147e-05, 2.373511523027522e-05, 1.8036621932984037e-05, 1.6878632927286987e-05, 2.0514215042392473e-05, 3.214749839793484e-05, 3.1224586669172496e-05, 3.1224586669172496e-05, 5.2514902178407995e-05, 3.1224586669172496e-05, 3.214749839793484e-05, 4.7449657619144665e-05, 3.612494157985406e-05, 2.4391106141331697e-05, 3.1224586669172496e-05, 3.450378602705493e-05, 2.2204583023172878e-05, 1.9962491863571e-05, 2.143974753706761e-05, 2.6261534914802107e-05, 2.373511523027522e-05, 2.3089812389082975e-05, 3.612494157985406e-05, 2.3089812389082975e-05, 2.3089812389082975e-05, 4.539124612473466e-05, 2.6987350991419605e-05, 3.450378602705493e-05, 1.7345124439290927e-05, 1.9962491863571e-05, 2.6261534914802107e-05, 3.1224586669172496e-05, 1.6878632927286987e-05, 4.287282705643613e-05, 2.6261534914802107e-05, 1.6878632927286987e-05, 2.3089812389082975e-05, 1.7551532819532563e-05, 3.1224586669172496e-05, 2.70377526705395e-05, 5.4896358681602544e-05, 5.2514902178407995e-05, 2.1808732252082407e-05, 2.746008010191923e-05, 2.0306052549559947e-05, 5.4896358681602544e-05, 2.671350408839596e-05, 3.2087571527587897e-05, 2.7924913524488957e-05, 1.689771767343007e-05, 2.286088859164965e-05, 1.9205310654971643e-05, 2.6261534914802107e-05, 3.1224586669172496e-05, 1.9962491863571e-05, 2.373511523027522e-05, 3.0382993525136288e-05, 2.6261534914802107e-05, 2.7503080792828757e-05, 3.1224586669172496e-05, ... ]\n",
      "Split on feature emp_length.n/a. (35781, 1443)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (35781 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (1443 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "=====================================================\n",
      "Adaboost Iteration 29\n",
      "=====================================================\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 1 (37224 data points).\n",
      "[2.019462171711352e-05, 1.7566659024846763e-05, 2.4011114281890267e-05, 2.6566911933625063e-05, 3.158767554504915e-05, 3.158767554504915e-05, 3.158767554504915e-05, 2.6566911933625063e-05, 3.65450132550474e-05, 2.7771673678761494e-05, 1.7074902742507754e-05, 1.7755627316887366e-05, 2.3358307665365748e-05, 2.0542177429989223e-05, 3.2521319168701894e-05, 2.3358307665365748e-05, 3.302011990326191e-05, 1.7074902742507754e-05, 3.0736296103059625e-05, 4.591906917343307e-05, 3.158767554504915e-05, 2.019462171711352e-05, 3.0736296103059625e-05, 3.7648548212870725e-05, 2.6566911933625063e-05, 2.3358307665365748e-05, 2.3358307665365748e-05, 2.3358307665365748e-05, 3.158767554504915e-05, 3.65450132550474e-05, 2.6566911933625063e-05, 3.65450132550474e-05, 2.0542177429989223e-05, 2.7779394164306235e-05, 3.2521319168701894e-05, 3.2521319168701894e-05, 3.158767554504915e-05, 4.591906917343307e-05, 2.019462171711352e-05, 2.4011114281890267e-05, 4.718818001320363e-05, 2.4011114281890267e-05, 1.824635719230534e-05, 1.7074902742507754e-05, 2.075276049883523e-05, 3.2521319168701894e-05, 3.158767554504915e-05, 3.158767554504915e-05, 5.3125561240151654e-05, 3.158767554504915e-05, 3.2521319168701894e-05, 4.8001416495193345e-05, 3.65450132550474e-05, 2.467473325236685e-05, 3.158767554504915e-05, 3.490500641836985e-05, 2.2462784586402963e-05, 2.019462171711352e-05, 2.1196105985022257e-05, 2.6566911933625063e-05, 2.4011114281890267e-05, 2.3358307665365748e-05, 3.65450132550474e-05, 2.3358307665365748e-05, 2.3358307665365748e-05, 4.591906917343307e-05, 2.730116801766826e-05, 3.490500641836985e-05, 1.7546818757980516e-05, 2.019462171711352e-05, 2.6566911933625063e-05, 3.158767554504915e-05, 1.7074902742507754e-05, 4.238561972777975e-05, 2.6566911933625063e-05, 1.7074902742507754e-05, 2.3358307665365748e-05, 1.7755627316887366e-05, 3.158767554504915e-05, 2.7352155782657212e-05, 5.553471003512433e-05, 5.3125561240151654e-05, 2.156089755324315e-05, 2.7779394164306235e-05, 2.0542177429989223e-05, 5.553471003512433e-05, 2.7024136740573164e-05, 3.246069545070026e-05, 2.824963280958264e-05, 1.7094209411813403e-05, 2.3126721873230874e-05, 1.9428635777909463e-05, 2.6566911933625063e-05, 3.158767554504915e-05, 2.019462171711352e-05, 2.4011114281890267e-05, 3.0736296103059625e-05, 2.6566911933625063e-05, 2.7822894880170128e-05, 3.158767554504915e-05, ... ]\n",
      "Split on feature grade.C. (27812, 9412)\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (27812 data points).\n",
      "Reached maximum depth. Stopping for now.\n",
      "--------------------------------------------------------------------\n",
      "Subtree, depth = 2 (9412 data points).\n",
      "Reached maximum depth. Stopping for now.\n"
     ]
    }
   ],
   "source": [
    "# this may take a while... \n",
    "stump_weights, tree_stumps = adaboost_with_tree_stumps(train_data, \n",
    "                                 features, target, num_tree_stumps=30)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Computing training error at the end of each iteration\n",
    "\n",
    "Now, we will compute the classification error on the **train_data** and see how it is reduced as trees are added."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, training error = 0.421636578551\n",
      "Iteration 2, training error = 0.433430045132\n",
      "Iteration 3, training error = 0.400037610144\n",
      "Iteration 4, training error = 0.400037610144\n",
      "Iteration 5, training error = 0.384724908661\n",
      "Iteration 6, training error = 0.384617451107\n",
      "Iteration 7, training error = 0.382763808296\n",
      "Iteration 8, training error = 0.384617451107\n",
      "Iteration 9, training error = 0.382763808296\n",
      "Iteration 10, training error = 0.384483129164\n",
      "Iteration 11, training error = 0.382736943907\n",
      "Iteration 12, training error = 0.381447453256\n",
      "Iteration 13, training error = 0.381528046422\n",
      "Iteration 14, training error = 0.380560928433\n",
      "Iteration 15, training error = 0.380507199656\n",
      "Iteration 16, training error = 0.378223726628\n",
      "Iteration 17, training error = 0.378277455405\n",
      "Iteration 18, training error = 0.378411777348\n",
      "Iteration 19, training error = 0.378062540297\n",
      "Iteration 20, training error = 0.378761014399\n",
      "Iteration 21, training error = 0.379566946056\n",
      "Iteration 22, training error = 0.378895336342\n",
      "Iteration 23, training error = 0.378895336342\n",
      "Iteration 24, training error = 0.378761014399\n",
      "Iteration 25, training error = 0.378895336342\n",
      "Iteration 26, training error = 0.378975929508\n",
      "Iteration 27, training error = 0.379110251451\n",
      "Iteration 28, training error = 0.378922200731\n",
      "Iteration 29, training error = 0.379029658285\n",
      "Iteration 30, training error = 0.378734150011\n"
     ]
    }
   ],
   "source": [
    "error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], train_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(train_data[target], predictions)\n",
    "    error_all.append(error)\n",
    "    print \"Iteration %s, training error = %s\" % (n, error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualizing training error vs number of iterations\n",
    "\n",
    "We have provided you with a simple code snippet that plots classification error with the number of iterations. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAdcAAAFgCAYAAADpSzMMAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b2c24e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "\n",
    "plt.rcParams.update({'font.size': 16})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Quiz Question**: Which of the following best describes a **general trend in accuracy** as we add more and more components? Answer based on the 30 components learned so far.\n",
    "\n",
    "1. Training error goes down monotonically, i.e. the training error reduces with each iteration but never increases.\n",
    "2. Training error goes down in general, with some ups and downs in the middle.\n",
    "3. Training error goes up in general, with some ups and downs in the middle.\n",
    "4. Training error goes down in the beginning, achieves the best error, and then goes up sharply.\n",
    "5. None of the above\n",
    "\n",
    "\n",
    "### Evaluation on the test data\n",
    "\n",
    "Performing well on the training data is cheating, so lets make sure it works on the `test_data` as well. Here, we will compute the classification error on the `test_data` at the end of each iteration."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Iteration 1, test error = 0.42330891857\n",
      "Iteration 2, test error = 0.428479103835\n",
      "Iteration 3, test error = 0.398104265403\n",
      "Iteration 4, test error = 0.398104265403\n",
      "Iteration 5, test error = 0.379900904782\n",
      "Iteration 6, test error = 0.380008616975\n",
      "Iteration 7, test error = 0.379254631624\n",
      "Iteration 8, test error = 0.380008616975\n",
      "Iteration 9, test error = 0.379254631624\n",
      "Iteration 10, test error = 0.379685480396\n",
      "Iteration 11, test error = 0.379254631624\n",
      "Iteration 12, test error = 0.377962085308\n",
      "Iteration 13, test error = 0.379254631624\n",
      "Iteration 14, test error = 0.377854373115\n",
      "Iteration 15, test error = 0.378500646273\n",
      "Iteration 16, test error = 0.377854373115\n",
      "Iteration 17, test error = 0.377962085308\n",
      "Iteration 18, test error = 0.377854373115\n",
      "Iteration 19, test error = 0.378177509694\n",
      "Iteration 20, test error = 0.376884963378\n",
      "Iteration 21, test error = 0.377531236536\n",
      "Iteration 22, test error = 0.376777251185\n",
      "Iteration 23, test error = 0.376777251185\n",
      "Iteration 24, test error = 0.376884963378\n",
      "Iteration 25, test error = 0.376777251185\n",
      "Iteration 26, test error = 0.376561826799\n",
      "Iteration 27, test error = 0.376454114606\n",
      "Iteration 28, test error = 0.376992675571\n",
      "Iteration 29, test error = 0.376777251185\n",
      "Iteration 30, test error = 0.376777251185\n"
     ]
    }
   ],
   "source": [
    "test_error_all = []\n",
    "for n in xrange(1, 31):\n",
    "    predictions = predict_adaboost(stump_weights[:n], tree_stumps[:n], test_data)\n",
    "    error = 1.0 - graphlab.evaluation.accuracy(test_data[target], predictions)\n",
    "    test_error_all.append(error)\n",
    "    print \"Iteration %s, test error = %s\" % (n, test_error_all[n-1])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Visualize both the training and test errors\n",
    "\n",
    "Now, let us plot the training & test error with the number of iterations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAeIAAAFSCAYAAAAuI9zWAAAABHNCSVQICAgIfAhkiAAAAAlwSFlz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\n1lSpAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<matplotlib.figure.Figure at 0x1b30db70>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "plt.rcParams['figure.figsize'] = 7, 5\n",
    "plt.plot(range(1,31), error_all, '-', linewidth=4.0, label='Training error')\n",
    "plt.plot(range(1,31), test_error_all, '-', linewidth=4.0, label='Test error')\n",
    "\n",
    "plt.title('Performance of Adaboost ensemble')\n",
    "plt.xlabel('# of iterations')\n",
    "plt.ylabel('Classification error')\n",
    "plt.rcParams.update({'font.size': 16})\n",
    "plt.legend(loc='best', prop={'size':15})\n",
    "plt.tight_layout()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "** Quiz Question:** From this plot (with 30 trees), is there massive overfitting as the # of iterations increases?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
